{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8477a4e5-612f-4f2a-8063-96f7580f3afa",
   "metadata": {},
   "source": [
    "### Document Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "658121de-3146-45f7-965c-fe3ec5114920",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "import os, codecs, string, random\n",
    "import numpy as np\n",
    "from numpy.random import seed as random_seed\n",
    "from numpy.random import shuffle as random_shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "#NLP libraries\n",
    "import spacy, nltk, gensim, sklearn\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "\n",
    "#Scikit imports\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca14192-61e8-4b81-8fbf-750eb1a2af48",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1: obtain a large collection of documents and labels it classes : recover sample fof quotation from quotabank and manualy label it climate or not \n",
    "\n",
    "We already have quotations related to climates, now we will extract some random quotation from each years and then construte manually a set of quotation related to climate or not each coming from different years. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "044ee9d0-3811-4ec4-b6e0-da1d103a353b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dico={} #We create a dictonary to loop over our years. \n",
    "for date in [2020, 2019, 2018, 2017, 2016, 2015]:\n",
    "    dico[date] = pd.read_json(f'data/quotes-{date}.json.bz2', lines=True, compression='bz2', chunksize=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8242dd21-61d8-4ac4-a79d-4df5a01f77fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_filtering(chunk, i):\n",
    "    \n",
    "    template=[] #Creation of an empty list :it's always cheaper to append to a list and create a DataFrame than append on a empty dataframe\n",
    "    template.append(chunk.loc[i:i]) #selecte one quotation per chunk \n",
    "    return(pd.concat(template, ignore_index=True))# return a dataframe with our data of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "04e6d84d-4d93-4442-8dab-8df120561c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "for date, df in dico.items() : \n",
    "    m=1\n",
    "    for i, chunk in enumerate(df) : \n",
    "        chunk_clean=chunk_filtering(chunk, m) #recover interested row of the chunk\n",
    "        header = i == 0 #we kept the name of the column only for the first chunk\n",
    "        mode = 'w' if i == 0 else 'a' # For appending data to an existing CSV file (so for every chunk exepct the first one), \n",
    "                                        #we can use mode = a\n",
    "        m+=100000    \n",
    "        chunk_clean.to_csv(path_or_buf=f\"data/clean_quotes-sample-{date}.bz2\",compression='bz2',header=header, mode=mode, index = False ) #Load to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b38cdd5-78b6-4dad-bb4f-726bc2527305",
   "metadata": {},
   "outputs": [],
   "source": [
    " df_reader =  pd.read_json('data/quotes-2020.json.bz2', lines=True, compression='bz2', chunksize=100000)\n",
    "m=1\n",
    "for i, chunk in enumerate(df_reader) :\n",
    "    chunk_clean=chunk_filtering(chunk, m) #recover interested row of the chunk\n",
    "    header = i == 0 #we kept the name of the column only for the first chunk\n",
    "    mode = 'w' if i == 0 else 'a' # For appending data to an existing CSV file (so for every chunk exepct the first one), \n",
    "                                        #we can use mode = a\n",
    "    m+=100000\n",
    "    chunk_clean.to_csv(path_or_buf=\"data/clean_quotes-2020-Sample.bz2\",compression='bz2',header=header, mode=mode, index = False ) #Load to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7906801d-153c-48fd-a9ca-430cc0bfe3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dico_random_quotes={} \n",
    "length = 0\n",
    "for date in [2020, 2018, 2017, 2016, 2015]:\n",
    "    dico_random_quotes[date] = pd.read_csv(f'data/clean_quotes-sample-{date}.bz2', compression='bz2')\n",
    "    length += len(dico_random_quotes[date]) #The length iss used here to obtain the total number of quotes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fece184f-1050-4822-b53b-22beafd9cf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "dico_clean={} \n",
    "length = 0\n",
    "for date in [2020, 2019, 2018, 2017, 2016, 2015]:\n",
    "    dico_clean[date] = pd.read_csv(f'data/clean_quotes-{date}.bz2', compression='bz2')\n",
    "    length += len(dico_clean[date]) #The length is used here to obtain the total number of quotes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfe22956-ddbc-4d5c-91ad-01a10d763ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131472\n"
     ]
    }
   ],
   "source": [
    "# create a corpus based on the csv file \n",
    "print(length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b568a4-34bd-44f2-a2ef-e45ac92f80d3",
   "metadata": {},
   "source": [
    "Let's create a subdata containing quotations from Quotebank data and label it climate or not. To do so, we decide to groups together random quotes and pre-selecte climate related quotes. Then, we will filtering the duplicates (to be sure that random quotes does not have quotes about climate) and assigne 1 to climate if the quotes comes from the pre-selected data from Milestone 2 or 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "999ae4cf-7b21-4c88-8b07-0f46374d0914",
   "metadata": {},
   "outputs": [],
   "source": [
    "m =[]\n",
    "#for date , data in dico_clean.items() : m.append(data.sample(30)) #add quotation about the climate\n",
    "    \n",
    "for date, data in dico_random_quotes.items() : m.append(data.sample(80, replace=True)) #add random quotation\n",
    "\n",
    "subdata =pd.concat(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0dab13-37a5-4ef5-8f0c-c4e74cf6563e",
   "metadata": {},
   "source": [
    "Filtering our subdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "597bba2b-7091-4056-b374-0f7ad6b3a31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "subdata.drop_duplicates(subset=['quotation'], inplace=True) \n",
    "subdata.dropna(subset=['quotation'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cec5edf7-1705-4f34-8a2e-bb407aeede14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(305, 9)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd0f146b-92f7-4d30-8b3f-641b84df8e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quoteID</th>\n",
       "      <th>quotation</th>\n",
       "      <th>speaker</th>\n",
       "      <th>qids</th>\n",
       "      <th>date</th>\n",
       "      <th>numOccurrences</th>\n",
       "      <th>probas</th>\n",
       "      <th>urls</th>\n",
       "      <th>phase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [quoteID, quotation, speaker, qids, date, numOccurrences, probas, urls, phase]\n",
       "Index: []"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdata[subdata['quotation'].apply(lambda x : '' in x)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8e17ad-8713-4354-a1e9-a8b2443a0583",
   "metadata": {},
   "source": [
    "Mask quotation to climate or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e548022-8a8d-4d50-a29c-d31fa7d38f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "subdata['climate']=0\n",
    "subdata.iloc[:179]['climate']=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f98b4c-2c61-4586-b0b0-54ecde667150",
   "metadata": {},
   "source": [
    "Let's see if quotation related to climate are ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77890b39-39d7-41df-91b6-742abc65cbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "subdata.to_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b50f59e-2dda-4238-9df8-73f362c5f809",
   "metadata": {},
   "source": [
    "Regarding ourself the quotations it seems that they all abord climate subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a28af7c8-d799-4b8a-a902-f9f0832932d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "subdata_train, subdata_test, label_train, label_test = train_test_split(subdata.quotation, subdata.climate, shuffle=True, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1685aaa3-3624-44bc-9879-c2f333e17e6f",
   "metadata": {},
   "source": [
    "#### Create our bag of word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b0434717-4af8-4fe3-a710-52d09d92e253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 430\n",
      "Number of features: 8306\n",
      "(430,)\n",
      "(430, 8306)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words = 'english',ngram_range=(1, 2),lowercase=False)\n",
    "\n",
    "#initialize and specify minumum number of occurences to avoid untractable number of features\n",
    "#vectorizer = CountVectorizer(min_df = 2) if we want high frequency\n",
    "\n",
    "#create bag of words features\n",
    "X = vectorizer.fit_transform(subdata.quotation)\n",
    "\n",
    "\n",
    "print('Number of samples:',X.toarray().shape[0])\n",
    "print('Number of features:',X.toarray().shape[1])\n",
    "\n",
    "#mask and convert to int Frankenstein\n",
    "Y = np.array(subdata.climate)\n",
    "\n",
    "\n",
    "\n",
    "print(Y.shape)\n",
    "print(X.shape)\n",
    "#shuffle the data\n",
    "\n",
    "X, Y = shuffle(X, Y, random_state=0)\n",
    "\n",
    "#split into training and test set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f416ed06-d250-4799-bccd-46d301ae2c4b",
   "metadata": {},
   "source": [
    "--> more features than sample, may induce high variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0e7bb22a-c3fa-4c2f-8b68-28f59e35cc7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<86x8306 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2260 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train)\n",
    "X_test_tfidf=tfidf_transformer.fit_transform(X_test)\n",
    "#transform the count matrix X_train to a normalized tf-idf representation\n",
    "X_train_tfidf.shape \n",
    "X_test_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7affff00-1cca-472a-b31b-4a2f3d9a7044",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [X_train, X_train_tfidf]\n",
    "test = [X_test, X_test_tfidf]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5aede09-07b4-47b6-bbb1-dad4167bd4da",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3  : fit to different models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faa58ef-eb4b-4095-b984-12b11c954659",
   "metadata": {},
   "source": [
    "Text files are actually series of words. In order to run machine learning algorithms we need to convert the text files into numerical feature vectors. We will be using bag of words model for our example. Briefly, we segment each text file into words (for English splitting by space), and count # of times each word occurs in each document and finally assign each word an integer id. Each unique word in our dictionary will correspond to a feature (descriptive feature).\n",
    "Scikit-learn has a high level component which will create feature vectors for us ‘CountVectorizer’. \n",
    "Then we will train our feature with different model and seek for the better. \n",
    "\n",
    "To do so, we need first to find the best parameters for CountVectorize, decide if we need to transform our feature (with TF-IDF), and check for hyperparameters tunning with cross validation. \n",
    "We will use Pipeline and GridSearchCSV. \n",
    "\n",
    "NB : we don't look for n_grams = (1,1) because we thought that a list of simple word cannot underligne the subject an can induce biaise (for exemple : it's better for us to get 'y.. than ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7baf4f3-0fd5-412a-af90-c3eaa683a7f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Let's fit some models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc049890-40e1-4d1d-925c-2aed5d3204ec",
   "metadata": {},
   "source": [
    "##### Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "cb1c4545-cb51-4da2-8ec0-1dc5ad5bc595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 24 candidates, totalling 240 fits\n",
      "Best Score:  0.9955340136054422\n",
      "Best Params:  {'C': 10, 'penalty': 'l2', 'random_state': 0, 'solver': 'lbfgs'}\n",
      "Fitting 10 folds for each of 24 candidates, totalling 240 fits\n",
      "Best Score:  0.9934319727891158\n",
      "Best Params:  {'C': 0.0001, 'penalty': 'l2', 'random_state': 0, 'solver': 'lbfgs'}\n"
     ]
    }
   ],
   "source": [
    "penalty = ['l1', 'l2', 'elasticnet'] \n",
    "C = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000] \n",
    "random_state=[0]\n",
    "solver=['lbfgs']\n",
    "\n",
    "param_grid = dict(penalty=penalty, \n",
    "C=C, random_state=random_state, solver=solver) \n",
    "\n",
    "logistic = LogisticRegression() \n",
    "\n",
    "grid = GridSearchCV(estimator=logistic, param_grid=param_grid, scoring='roc_auc', verbose=1, n_jobs=-1, cv = 10) \n",
    "\n",
    "for features in (train) : \n",
    "    grid_result = grid.fit(features, Y_train) \n",
    "    print('Best Score: ', grid_result.best_score_) \n",
    "    print('Best Params: ', grid_result.best_params_) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba31bc7c-868b-4ea8-8484-dc561e32704f",
   "metadata": {},
   "source": [
    "We can see that X_train seems to give a better accuracy than tf_idf transfrom vector and that the best parameters for c equals 1. However, as we get many features compare to the number of document a simple logistic rgression may induce an high variance and so it may not be the best option"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2120d67-6e5c-4392-a7ff-ac888202e99e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "220b183b-bfe7-4f70-b91d-e7964daa54af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 90 candidates, totalling 900 fits\n",
      "Best Score:  0.9941394557823129\n",
      "Best Params:  {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 10, 'n_jobs': -1, 'penalty': 'l2'}\n",
      "Fitting 10 folds for each of 90 candidates, totalling 900 fits\n",
      "Best Score:  0.9951428571428572\n",
      "Best Params:  {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 10000, 'n_jobs': -1, 'penalty': 'elasticnet'}\n"
     ]
    }
   ],
   "source": [
    "penalty = ['l1', 'l2', 'elasticnet'] \n",
    "alpha = [1e-4, 1e-3, 1e-2, 1e-1, 1e0]\n",
    "loss=['log', 'hinge']\n",
    "max_iter=[10,100,10000]\n",
    "n_jobs = [-1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "param_grid = dict(penalty=penalty, \n",
    "alpha=alpha,loss=loss,max_iter=max_iter, n_jobs=n_jobs) \n",
    "\n",
    "logistic = SGDClassifier() \n",
    "\n",
    "grid = GridSearchCV(estimator=logistic, param_grid=param_grid, scoring='roc_auc', verbose=1, n_jobs=-1, cv = 10) \n",
    "\n",
    "for features in (train) : \n",
    "    grid_result = grid.fit(features, Y_train) \n",
    "    print('Best Score: ', grid_result.best_score_) \n",
    "    print('Best Params: ', grid_result.best_params_) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4bb0ad-3eb1-4bed-960f-86a14ab60a34",
   "metadata": {},
   "source": [
    "#### Let's get our model and see what word reflect climate topic !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "92d79a17-1ed3-4aaf-8bb1-1d722df89a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.9418604651162791\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0, solver='lbfgs',C = 10, penalty='l2').fit(X_train,Y_train)\n",
    "predicted = clf.predict(X_test)\n",
    "print('Accuracy:{}'.format(np.mean(predicted == Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "da696327-b50d-4faf-92f7-972769f974d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['long' 'business' 'global warming' 'greenhouse' 'fossil fuels' 'fuels'\n",
      " 'fossil' 'warming' 'climate' 'renewable energy' 'business usual'\n",
      " 'renewable' 'climate change' 'change' 'global' 'energy' 'emissions'\n",
      " 'usual' 'CO2' 'carbon']\n"
     ]
    }
   ],
   "source": [
    "coefs=clf.coef_[0]\n",
    "top_three = np.argpartition(coefs, -20)[-20:]\n",
    "print(np.array(vectorizer.get_feature_names())[top_three])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c631cd04-4298-4a41-8a3c-440466e37cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gas' 'greenhouse gases' 'gases' 'need' 'future' 'long' 'use'\n",
      " 'global warming' 'climate' 'fossil' 'business usual' 'business'\n",
      " 'greenhouse' 'climate change' 'fuels' 'usual' 'warming' 'energy' 'CO2'\n",
      " 'global' 'change' 'renewable' 'emissions' 'carbon' 'renewable energy'\n",
      " 'fossil fuels']\n"
     ]
    }
   ],
   "source": [
    "coefs=clf.coef_[0]\n",
    "top_three = np.argpartition(coefs, -20)[-26:]\n",
    "print(np.array(vectorizer.get_feature_names())[top_three])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4d5f94-e886-4c68-a2ec-8ba3b6f826e9",
   "metadata": {},
   "source": [
    "##### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "302ee168-f9ee-4610-9121-bb4b173cced3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.9651162790697675\n",
      "['deliver' 'long' 'future' 'fight global' 'gas' 'lead' 'gases'\n",
      " 'greenhouse gases' 'We need' 'environment' 'use' 'global warming'\n",
      " 'global' 'greenhouse' 'fossil' 'business usual' 'fossil fuels' 'warming'\n",
      " 'fuels' 'business' 'emissions' 'usual' 'CO2' 'renewable' 'energy'\n",
      " 'climate' 'change' 'climate change' 'renewable energy' 'carbon']\n"
     ]
    }
   ],
   "source": [
    "clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", alpha = 0.01,   max_iter=10, n_jobs=-1).fit(X_train, Y_train)\n",
    "predicted = clf.predict(X_test)\n",
    "print('Accuracy:{}'.format(np.mean(predicted == Y_test)))\n",
    "\n",
    "\n",
    "\n",
    "coefs=clf.coef_[0]\n",
    "top_three = np.argpartition(coefs, -18)[-30:]\n",
    "\n",
    "print(np.array(vectorizer.get_feature_names())[top_three])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "acc128a2-9994-4624-b8b4-94ab1ff3fd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.9302325581395349\n",
      "['gas' 'content share' 'business' 'greenhouse gas' 'greenhouse gases'\n",
      " 'global warming' 'contend' 'gases' 'content post' 'content' 'need'\n",
      " 'future' 'warming' 'global' 'use' 'fossil fuels' 'carbon' 'change' 'CO2'\n",
      " 'emissions' 'fuels' 'usual' 'fossil' 'energy' 'business usual' 'climate'\n",
      " 'renewable' 'renewable energy' 'greenhouse' 'climate change']\n"
     ]
    }
   ],
   "source": [
    "clf = SGDClassifier(loss=\"hinge\", penalty=\"elasticnet\", alpha = 1e-3,   max_iter=10000, n_jobs=-1).fit(X_train_tfidf, Y_train)\n",
    "predicted = clf.predict(X_test_tfidf)\n",
    "print('Accuracy:{}'.format(np.mean(predicted == Y_test)))\n",
    "\n",
    "\n",
    "\n",
    "coefs=clf.coef_[0]\n",
    "top_three = np.argpartition(coefs, -18)[-30:]\n",
    "\n",
    "print(np.array(vectorizer.get_feature_names())[top_three])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0f84c907-1e12-4895-bd77-f16e1a2fa593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['carbon dioxide' 'national' 'gases' 'greenhouse gas' 'greenhouse gases'\n",
      " 'gas' 'future' 'global' 'warming' 'emissions' 'fuels' 'CO2'\n",
      " 'fossil fuels' 'fossil' 'use' 'usual' 'business usual' 'greenhouse'\n",
      " 'renewable energy' 'renewable' 'carbon' 'climate change' 'climate'\n",
      " 'change' 'energy']\n"
     ]
    }
   ],
   "source": [
    "oefs=clf.coef_[0]\n",
    "top_three = np.argpartition(coefs, -20)[-25:]\n",
    "\n",
    "print(np.array(vectorizer.get_feature_names())[top_three])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "67c576c2-3b28-4204-9568-e104e5e04bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 24 candidates, totalling 240 fits\n",
      "Best Score:  0.9955340136054422\n",
      "Best Params:  {'C': 10, 'penalty': 'l2'}\n",
      "Fitting 10 folds for each of 24 candidates, totalling 240 fits\n",
      "Best Score:  0.9934319727891158\n",
      "Best Params:  {'C': 0.0001, 'penalty': 'l2'}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e47f1c-8ead-4f15-97d5-a24654e5a11e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba78a7d-db01-4ade-9c59-35d4a1ee8515",
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty = ['l1', 'l2', 'elasticnet'] \n",
    "C = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000] \n",
    "\n",
    "\n",
    "param_grid = dict(penalty=penalty, \n",
    "C=C) \n",
    "\n",
    "logistic = LogisticRegression() \n",
    "\n",
    "grid = GridSearchCV(estimator=logistic, param_grid=param_grid, scoring='roc_auc', verbose=1, n_jobs=-1) \n",
    "\n",
    "grid_result = grid.fit(X_train, Y_train) \n",
    "\n",
    "print('Best Score: ', grid_result.best_score_) \n",
    "print('Best Params: ', grid_result.best_params_) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51af2255-7db7-4a18-b944-e8ce10fdb373",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ae0e3bf-328c-428d-97f4-2d6eed6b7780",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# implementing train-test-split\n",
    "X_train, X_test, y_train, y_test = train_test_split(subdata.quotation, subdata.climate, test_size=0.33, random_state=66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47a2b9d7-1e6f-490e-87b1-d9f4000e4731",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'To my knowledge, these kind of symbiont-engulfing worms are more widespread around deep-ocean seeps than other seep-associated organisms and the nature of how they acquire nutrients through that symbiosis would extend the known habitat for seep ecosystems and our appreciation for how methane supports deep-sea ecosystems.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/dp/xm3y4vf11sd5mzn0yd618jhm0000gn/T/ipykernel_90835/2147366144.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# random forest model creation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mrfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mrfc_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ada/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         X, y = self._validate_data(\n\u001b[0m\u001b[1;32m    328\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ada/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    574\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ada/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y cannot be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 956\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m    957\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ada/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    736\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 738\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    739\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m                 raise ValueError(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ada/lib/python3.8/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order, like)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_asarray_with_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ada/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    855\u001b[0m               dtype='datetime64[ns]')\n\u001b[1;32m    856\u001b[0m         \"\"\"\n\u001b[0;32m--> 857\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[0;31m# ----------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ada/lib/python3.8/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order, like)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_asarray_with_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'To my knowledge, these kind of symbiont-engulfing worms are more widespread around deep-ocean seeps than other seep-associated organisms and the nature of how they acquire nutrients through that symbiosis would extend the known habitat for seep ecosystems and our appreciation for how methane supports deep-sea ecosystems.'"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# random forest model creation\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train,y_train)\n",
    "# predictions\n",
    "rfc_predict = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8b2083-8118-4356-9ca5-682c2b4a512e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
