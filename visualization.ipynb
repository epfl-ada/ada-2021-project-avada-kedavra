{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4f3bfb-1250-4fe3-9fd7-6c5ccc66a943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import ast\n",
    "from ast import literal_eval\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661de6df-3ac3-4334-89f1-9653b26ff97d",
   "metadata": {},
   "source": [
    "# ORGANISATION\n",
    "- classification - alicia / maria \n",
    "- temporal evolution - anissa \n",
    "- majoritiy representation - virginie\n",
    "- climate speaking proprotion - virginie \n",
    "- sentiment analysis - alicia / maria \n",
    "- readme data,set  - virginie\n",
    "- readme methodes - alicia / maria \n",
    "- data story (site web) - anissa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49d1f72-7149-4a64-b458-c1c9aa4d4623",
   "metadata": {},
   "source": [
    "- [doing] CLASSIFICATION - Based on the evolution of the profile of a person talking about climate in the media, can we predict what characteristics will a climate-involved person have in the future ? \n",
    " \n",
    " - [doing] TEMPORAL EVOLUTION - Is there any temporal evolution in the mediatic coverage of the climate crisis ? If so, does it correlate with specific events ? \n",
    " \n",
    " - [doing] MAJORITY REPRESENTATION - Who are the top 10 people that talks the most about climate change over the year ? Does it show an evolution in the general opinions made publicly avaiblable by mainstream media ? What is the current typical profile of a person talking about climate in the media ? Is it the same for other topic ? (Comparaison entre les caractéristiques de personnes les plus représentées dans le QuoteBank vs celles qui parlent du climat) (comparaison des nationalités avec une carte du monde) \n",
    " \n",
    " - [x] CLIMATE SPEAKING PROPORTION - Are some communities excluded from the mediatic discussion surround climate change ? (Comparer la proportion de chacun des communautés dans QB en général avec la propotion des communautés dans les phrases qui parlent du climat) - plot climate vs non climate \n",
    " \n",
    " - [x] NLP TOPIC CLASSIFICATION - What is the most represented climate crisis-related topic along the years ? Does it show an evolution in the general opinions made publicly avaiblable by mainstream media ? (Trouver les mots reliés au climat les plus représentés dans les quotes chaque années) \n",
    "\n",
    "- [x] SENTIMENT ANALYSIS - Vander Analysis among people that talks about climate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140ddab1-34ea-4f5f-8406-bedf453d2ab0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# A RUN POUR AVOIR SUBDATA (Year-month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96facbb7-7c5f-453b-a330-31215b3e4f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dico_clean={} \n",
    "length = 0\n",
    "for date in [2020, 2019, 2018, 2017, 2016, 2015]:\n",
    "    dico_clean[date] = pd.read_csv(f'data/clean_quotes-{date}.bz2', compression='bz2')\n",
    "    length += len(dico_clean[date]) #The length is used here to obtain the total number of quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42406e88-3bcc-4c91-8be3-eb06bbd3178d",
   "metadata": {},
   "outputs": [],
   "source": [
    "subdata=pd.DataFrame()\n",
    "for years, quotes in dico_clean.items() : \n",
    "    subdata = pd.concat([subdata, quotes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c86d71-8c5c-4868-bdc0-fb09aa15ea42",
   "metadata": {},
   "outputs": [],
   "source": [
    "subdata.date.map(lambda y : datetime.strptime(y, '%Y-%m-%d %H:%M:%S'))\n",
    "subdata.sort_values(by='date', inplace=True)\n",
    "subdata['date'] = pd.to_datetime(subdata['date']).dt.strftime('%Y-%m')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c652e95a-f8f3-4f71-bae9-e8b756e72bdb",
   "metadata": {},
   "source": [
    "# I- CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d105c0c-86ac-437b-8b55-812c7be472b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.1 Logistic - baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53b9f25-9410-4348-8e61-d0efd4129205",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_original = pd.read_csv(\"data/one_hot_data.bz2\", compression = 'bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846748ed-b92c-48b4-b631-bd2c63126745",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_set(data_to_split, ratio=0.8):\n",
    "    mask = np.random.rand(len(data_to_split)) < ratio\n",
    "    return [data_to_split[mask].reset_index(drop=True), data_to_split[~mask].reset_index(drop=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174e0351-506b-41dc-85a4-f35fbaac8b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features = data_original.copy()\n",
    "data_features.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8b365f-0b3f-480a-9710-1f2a40dee7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "[train, test] = split_set(data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1230bcd1-652f-42b8-897d-d0268475bd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label=train.climate\n",
    "train_features = train.drop('climate', axis=1)\n",
    "print('Length of the train dataset : {}'.format(len(train)))\n",
    "\n",
    "test_label=test.climate\n",
    "test_features = test.drop('climate', axis=1)\n",
    "print('Length of the test dataset : {}'.format(len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5042ec3b-855a-44f5-b4af-2b34c2ede034",
   "metadata": {},
   "outputs": [],
   "source": [
    "means = train_features.mean()\n",
    "stddevs = train_features.std()\n",
    "\n",
    "train_features_std = pd.DataFrame()\n",
    "for c in train_features.columns:\n",
    "    train_features_std[c] = (train_features[c]-means[c])/stddevs[c]\n",
    "\n",
    "# Use the mean and stddev of the training set\n",
    "test_features_std = pd.DataFrame()\n",
    "for c in test_features.columns:\n",
    "    test_features_std[c] = (test_features[c]-means[c])/stddevs[c]\n",
    "\n",
    "train_features_std.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0876c98-ba0e-41f4-bbaf-e5e1ae310d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confusion_matrix(true_label, prediction_proba, decision_threshold=0.5): \n",
    "    \n",
    "    predict_label = (prediction_proba[:,1]>decision_threshold).astype(int)   \n",
    "                                                                                                                       \n",
    "    TP = np.sum(np.logical_and(predict_label==1, true_label==1))\n",
    "    TN = np.sum(np.logical_and(predict_label==0, true_label==0))\n",
    "    FP = np.sum(np.logical_and(predict_label==1, true_label==0))\n",
    "    FN = np.sum(np.logical_and(predict_label==0, true_label==1))\n",
    "    \n",
    "    confusion_matrix = np.asarray([[TP, FP],\n",
    "                                    [FN, TN]])\n",
    "    return confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(confusion_matrix):\n",
    "    [[TP, FP],[FN, TN]] = confusion_matrix\n",
    "    label = np.asarray([['TP {}'.format(TP), 'FP {}'.format(FP)],\n",
    "                        ['FN {}'.format(FN), 'TN {}'.format(TN)]])\n",
    "    \n",
    "    df_cm = pd.DataFrame(confusion_matrix, index=['Yes', 'No'], columns=['Positive', 'Negative']) \n",
    "    \n",
    "    return sns.heatmap(df_cm, cmap='YlOrRd', annot=label, annot_kws={\"size\": 16}, cbar=False, fmt='')\n",
    "\n",
    "def compute_all_score(confusion_matrix, t=0.5):\n",
    "    [[TP, FP],[FN, TN]] = confusion_matrix.astype(float)\n",
    "    \n",
    "    accuracy =  (TP+TN)/np.sum(confusion_matrix)\n",
    "    \n",
    "    precision_positive = TP/(TP+FP) if (TP+FP) !=0 else np.nan\n",
    "    precision_negative = TN/(TN+FN) if (TN+FN) !=0 else np.nan\n",
    "    \n",
    "    recall_positive = TP/(TP+FN) if (TP+FN) !=0 else np.nan\n",
    "    recall_negative = TN/(TN+FP) if (TN+FP) !=0 else np.nan\n",
    "\n",
    "    F1_score_positive = 2 *(precision_positive*recall_positive)/(precision_positive+recall_positive) if (precision_positive+recall_positive) !=0 else np.nan\n",
    "    F1_score_negative = 2 *(precision_negative*recall_negative)/(precision_negative+recall_negative) if (precision_negative+recall_negative) !=0 else np.nan\n",
    "\n",
    "    return [t, accuracy, precision_positive, recall_positive, F1_score_positive, precision_negative, recall_negative, F1_score_negative]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb6d400-803c-41aa-8563-14d63b0e53b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = LogisticRegression(solver='lbfgs', max_iter=10000)\n",
    "logistic.fit(train_features_std,train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09f5539-f39b-4959-8f09-df1931f2424b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_proba = logistic.predict_proba(test_features_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91562696-9647-47e8-84fc-a7be2732b131",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_05 = compute_confusion_matrix(test_label, prediction_proba, 0.5 )\n",
    "plt.figure(figsize = (4,3)) \n",
    "ax = plot_confusion_matrix(confusion_matrix_05)\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Confusion matrix for a 0.5 threshold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bd03e2-73c2-4b0b-9d63-19302013fa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "[t, accuracy, precision_positive, recall_positive, F1_score_positive, \\\n",
    "    precision_negative, recall_negative, F1_score_negative] = compute_all_score(confusion_matrix_05)\n",
    "\n",
    "print(\"The accuracy of this model is {0:1.3f}\".format(accuracy))\n",
    "print(\"For the positive case, the precision is {0:1.3f}, the recall is {1:1.3f} and the F1 score is {2:1.3f}\"\\\n",
    "      .format(precision_positive, recall_positive, F1_score_positive))\n",
    "print(\"For the negative case, the precision is {0:1.3f}, the recall is {1:1.3f} and the F1 score is {2:1.3f}\"\\\n",
    "      .format(precision_negative, recall_negative, F1_score_negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587ed67c-a050-43c2-aa45-05f170a189ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = []\n",
    "for name, value in zip(train_features_std.columns, logistic.coef_[0]):\n",
    "    tmp.append({\"name\": name, \"value\": value})\n",
    "    \n",
    "features_coef = pd.DataFrame(tmp).sort_values(\"value\")\n",
    "features_coef.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd79e1e6-0a04-459e-912a-c6f574127b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(6,40))\n",
    "plt.barh(features_coef.name, features_coef.value, alpha=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa435ff-6416-4d98-8810-dac43898ee22",
   "metadata": {},
   "source": [
    "### 1.2 Logistic - standardized vs not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5833f1a-5b37-4c67-8d3e-99ff54a0a0e6",
   "metadata": {},
   "source": [
    "### 1.2 Logistic - remove features vs not "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba52a1f-ea3f-4413-aeb1-80ccc8f45f58",
   "metadata": {},
   "source": [
    "### 2.1 Random forest "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c60597c-be61-4c19-bc2e-fa03edb1aa39",
   "metadata": {},
   "source": [
    "### 3.1 Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563afb1e-cb06-447c-8a1b-5af144126db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_booster = GradientBoostingClassifier(learning_rate=0.1)\n",
    "gradient_booster.fit(train_features,train_label)\n",
    "print(classification_report(test_label,gradient_booster.predict(test_feature)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642c8db6-ff99-422a-a984-c78984a10bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gradient_booster.feature_importances_)\n",
    "\n",
    "# plot\n",
    "pyplot.bar(range(len(gradient_booster.feature_importances_)), gradient_booster.feature_importances_)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060fcdfa-72ef-40e4-a036-a84b9d6d514f",
   "metadata": {},
   "source": [
    "# Temporal Evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd67b12f-635d-4243-9c46-d8eca604ced3",
   "metadata": {},
   "source": [
    "> Years and month "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbed301-fc6b-4b34-8bc4-8549d49e58df",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax=sns.catplot(data=subdata, x = 'date',  kind=\"count\", \n",
    "               palette=\"ch:.25\",height=7, aspect=2)\n",
    "ax.set_xticklabels(rotation=80)\n",
    "\n",
    "ax.set(xlabel='Time', ylabel='Number of quotes', \n",
    "       title= 'frequency of climate related quotes over the time ', yscale='log') #We chose a log scale for better data visulalisation. \n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf1d76c-a75b-4c94-8d1e-a578f7c0dba0",
   "metadata": {},
   "source": [
    "> Years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd654af-cf8d-439a-9d92-249d732901fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "subdata_years = subdata.copy()\n",
    "subdata_years['date'] = pd.to_datetime(subdata_years['date']).dt.strftime('%Y')\n",
    "\n",
    "\n",
    "ax=sns.catplot(data=subdata_years, x = 'date',  kind=\"count\", \n",
    "               palette=\"ch:.25\",height=4, aspect=1.8)\n",
    "\n",
    "ax.set_xticklabels(rotation=80)\n",
    "\n",
    "ax.set(xlabel='Time', ylabel='Number of quotes', \n",
    "       title= 'frequency of climate related quotes over the years ', yscale='log') #We chose a log scale for better data visulalisation. \n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435bd1da-3e9d-4cd7-8fab-703fb93167a3",
   "metadata": {},
   "source": [
    "# Majority representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d3c1c7-0e4a-449f-9c4e-8e9cbc958804",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10 = subdata.copy()\n",
    "speaker_time = top_10[['speaker','date','numOccurrences']]\n",
    "count_top_10 = speaker_time.groupby('speaker')['numOccurrences'].agg(['count']).sort_values('count', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f680dcb-ff2d-4176-8e02-200ddb652193",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_top_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7bb313-e581-4b49-8786-db1e32e4bd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_list = speaker_time['speaker'].tolist()\n",
    "\n",
    "x = 0 \n",
    "big_ls = []\n",
    "\n",
    "for i in count_top_10.index:\n",
    "    ls = []\n",
    "    x = x+1\n",
    "    if i in speaker_list:\n",
    "        ls.append(speaker_time['date'])\n",
    "        big_ls.append(ls)\n",
    "\n",
    "count_top_10['date'] = big_ls\n",
    "count_top_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40695b5e-aa04-4fb0-8f97-196f6b39b71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_speaker = count_top_10.index.tolist()\n",
    "y_count = count_top_10['count']\n",
    "\n",
    "count_top_10.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a30e08-5c4e-4213-a068-744620f9712b",
   "metadata": {},
   "source": [
    "# Climate speaking proprotion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716a57cf-c57a-4651-979b-1a1d2ed0c1e8",
   "metadata": {},
   "source": [
    "# NLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e349c9-4b48-4770-8405-61d82233a703",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
