{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6630819-7845-4bfb-b746-6a7e780e0540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from urllib.request import urlopen\n",
    "import sqlite3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967b3b89-46e1-4377-96ab-830501273244",
   "metadata": {},
   "source": [
    "# I- Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edcfe19-c424-4435-86bd-b0eb11096498",
   "metadata": {},
   "source": [
    "### Load Quotebank data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6264bc75-8b74-44f3-9e45-c079a54bea3b",
   "metadata": {},
   "source": [
    "First, let's recover the quotation of interest : as project is based on the caracterisation of the speaker, we decide to pre-select the quotations that are related to a speaker (i.e speaker value is different from 'None'). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd22b34b-ce66-41f4-82ba-a234b2327cba",
   "metadata": {},
   "source": [
    "##### *2020 quotes extractions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54006733-37d4-49a3-b0b2-a72f200b825b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_filtering(chunk):\n",
    "    template=[] #creation of an empty list :it's always cheaper to append to a list and create a DataFrame than append on a empty dataframe.\n",
    "    template.append(chunk[chunk[\"speaker\"].apply(lambda x: x!= \"None\")]) #select the quotation with value in speaker column different from 'None'\n",
    "    return pd.concat(template, ignore_index=True) # return a dataframe with our data of interest\n",
    "    \n",
    "\n",
    "with pd.read_json('data/quotes-2020.json.bz2', lines=True, compression='bz2', chunksize=1000) as df_reader:\n",
    "    for chunk in df_reader:\n",
    "        chunk_clean=chunk_filtering(chunk)\n",
    "        chunk_clean.to_csv(path_or_buf='data/clean_quotes-2020.bz2', compression='bz2', mode = 'a') # create a new csv files compress with bz2 containing all the dataframe recover from the chunk; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9edd5dc6-4e47-4744-94cc-5d0428f06a49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>quoteID</th>\n",
       "      <th>quotation</th>\n",
       "      <th>speaker</th>\n",
       "      <th>qids</th>\n",
       "      <th>date</th>\n",
       "      <th>numOccurrences</th>\n",
       "      <th>probas</th>\n",
       "      <th>urls</th>\n",
       "      <th>phase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-01-16-000088</td>\n",
       "      <td>[ Department of Homeland Security ] was livid ...</td>\n",
       "      <td>Sue Myrick</td>\n",
       "      <td>['Q367796']</td>\n",
       "      <td>2020-01-16 12:00:13</td>\n",
       "      <td>1</td>\n",
       "      <td>[['Sue Myrick', '0.8867'], ['None', '0.0992'],...</td>\n",
       "      <td>['http://thehill.com/opinion/international/478...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-01-24-000168</td>\n",
       "      <td>[ I met them ] when they just turned 4 and 7. ...</td>\n",
       "      <td>Meghan King Edmonds</td>\n",
       "      <td>['Q20684375']</td>\n",
       "      <td>2020-01-24 20:37:09</td>\n",
       "      <td>4</td>\n",
       "      <td>[['Meghan King Edmonds', '0.5446'], ['None', '...</td>\n",
       "      <td>['https://people.com/parents/meghan-king-edmon...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2020-01-17-000357</td>\n",
       "      <td>[ The delay ] will have an impact [ on Slough ...</td>\n",
       "      <td>Dexter Smith</td>\n",
       "      <td>['Q5268447']</td>\n",
       "      <td>2020-01-17 13:03:00</td>\n",
       "      <td>1</td>\n",
       "      <td>[['Dexter Smith', '0.924'], ['None', '0.076']]</td>\n",
       "      <td>['http://www.sloughexpress.co.uk/gallery/sloug...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2020-04-02-000239</td>\n",
       "      <td>[ The scheme ] treats addiction as an illness ...</td>\n",
       "      <td>Barry Coppinger</td>\n",
       "      <td>['Q4864119']</td>\n",
       "      <td>2020-04-02 14:18:20</td>\n",
       "      <td>1</td>\n",
       "      <td>[['Barry Coppinger', '0.9017'], ['None', '0.09...</td>\n",
       "      <td>['http://www.theweek.co.uk/106479/why-police-a...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2020-03-19-000276</td>\n",
       "      <td>[ These ] actions will allow households who ha...</td>\n",
       "      <td>Ben Carson</td>\n",
       "      <td>['Q816459']</td>\n",
       "      <td>2020-03-19 19:14:00</td>\n",
       "      <td>1</td>\n",
       "      <td>[['Ben Carson', '0.9227'], ['None', '0.0773']]</td>\n",
       "      <td>['https://mortgageorb.com/hud-fha-suspend-fore...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            quoteID  \\\n",
       "0         0.0  2020-01-16-000088   \n",
       "1         1.0  2020-01-24-000168   \n",
       "2         2.0  2020-01-17-000357   \n",
       "3         3.0  2020-04-02-000239   \n",
       "4         4.0  2020-03-19-000276   \n",
       "\n",
       "                                           quotation              speaker  \\\n",
       "0  [ Department of Homeland Security ] was livid ...           Sue Myrick   \n",
       "1  [ I met them ] when they just turned 4 and 7. ...  Meghan King Edmonds   \n",
       "2  [ The delay ] will have an impact [ on Slough ...         Dexter Smith   \n",
       "3  [ The scheme ] treats addiction as an illness ...      Barry Coppinger   \n",
       "4  [ These ] actions will allow households who ha...           Ben Carson   \n",
       "\n",
       "            qids                 date numOccurrences  \\\n",
       "0    ['Q367796']  2020-01-16 12:00:13              1   \n",
       "1  ['Q20684375']  2020-01-24 20:37:09              4   \n",
       "2   ['Q5268447']  2020-01-17 13:03:00              1   \n",
       "3   ['Q4864119']  2020-04-02 14:18:20              1   \n",
       "4    ['Q816459']  2020-03-19 19:14:00              1   \n",
       "\n",
       "                                              probas  \\\n",
       "0  [['Sue Myrick', '0.8867'], ['None', '0.0992'],...   \n",
       "1  [['Meghan King Edmonds', '0.5446'], ['None', '...   \n",
       "2     [['Dexter Smith', '0.924'], ['None', '0.076']]   \n",
       "3  [['Barry Coppinger', '0.9017'], ['None', '0.09...   \n",
       "4     [['Ben Carson', '0.9227'], ['None', '0.0773']]   \n",
       "\n",
       "                                                urls phase  \n",
       "0  ['http://thehill.com/opinion/international/478...     E  \n",
       "1  ['https://people.com/parents/meghan-king-edmon...     E  \n",
       "2  ['http://www.sloughexpress.co.uk/gallery/sloug...     E  \n",
       "3  ['http://www.theweek.co.uk/106479/why-police-a...     E  \n",
       "4  ['https://mortgageorb.com/hud-fha-suspend-fore...     E  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes_2020= pd.read_csv('data/clean_quotes-2020.bz2', compression='bz2')\n",
    "quotes_2020.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae4e97a-4d31-4c3f-a9ce-0c710073745d",
   "metadata": {},
   "source": [
    "##### *2019 quotes extractions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d516035-0364-44b3-b0c0-89a39d738e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.read_json('data/quotes-2019.json.bz2', lines=True, compression='bz2', chunksize=100000) as df_reader:\n",
    "    for chunk in df_reader:\n",
    "        chunk_clean=chunk_filtering(chunk)\n",
    "        chunk_clean.to_csv(path_or_buf='data/clean_quotes-2019.bz2', compression='bz2', mode = 'a') # create a new csv files compress with bz2 containing all the dataframe recover from the chunk; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4947d82f-97dd-4d82-95ed-cd580d10f6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes_2019= pd.read_csv('data/clean_quotes-2019.bz2', compression='bz2')\n",
    "quotes_2019.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898e8ab3-00a4-4dc9-b89f-e3958ed2bc84",
   "metadata": {},
   "source": [
    "##### *2018 quotes extractions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84914c33-8155-49ac-8d6a-d06d3f904e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.read_json('data/quotes-2018.json.bz2', lines=True, compression='bz2', chunksize=100000) as df_reader:\n",
    "    for chunk in df_reader:\n",
    "        chunk_clean=chunk_filtering(chunk)\n",
    "        chunk_clean.to_csv(path_or_buf='data/clean_quotes-2018.bz2', compression='bz2', mode = 'a') # create a new csv files compress with bz2 containing all the dataframe recover from the chunk; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca002f8c-56f4-4940-b198-a214ce2951f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes_2018= pd.read_csv('data/clean_quotes-2018.bz2', compression='bz2')\n",
    "quotes_2018.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8650499-8b64-4279-8cec-22c294b1f67c",
   "metadata": {},
   "source": [
    "##### *2017 quotes extractions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c3dec8-5fa1-474c-9141-ea4e2fc85608",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.read_json('data/quotes-2017.json.bz2', lines=True, compression='bz2', chunksize=100000) as df_reader:\n",
    "    for chunk in df_reader:\n",
    "        chunk_clean=chunk_filtering(chunk)\n",
    "        chunk_clean.to_csv(path_or_buf='data/clean_quotes-2017.bz2', compression='bz2', mode = 'a') # create a new csv files compress with bz2 containing all the dataframe recover from the chunk; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc42032-a525-4e90-9247-3a2e591aa26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes_2017= pd.read_csv('data/clean_quotes-2018.bz2', compression='bz2')\n",
    "quotes_2017.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3274154-6ca0-4d95-a254-92ee85f4fd9a",
   "metadata": {},
   "source": [
    "##### *2016 quotes extractions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dec2729-c0b5-419a-be2a-e39df7639e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.read_json('data/quotes-2016.json.bz2', lines=True, compression='bz2', chunksize=100000) as df_reader:\n",
    "    for chunk in df_reader:\n",
    "        chunk_clean=chunk_filtering(chunk)\n",
    "        chunk_clean.to_csv(path_or_buf='data/clean_quotes-2016.bz2', compression='bz2', mode = 'a') # create a new csv files compress with bz2 containing all the dataframe recover from the chunk; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fc23d3-9713-4206-a126-54882bc6fc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes_2016= pd.read_csv('data/clean_quotes-2016.bz2', compression='bz2')\n",
    "quotes_2016.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf4b6c0-579e-49e4-ae1f-8096fdbf0821",
   "metadata": {},
   "source": [
    "##### *2015 quotes extractions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8131f70-68ac-4126-a31d-f37d8944f444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13829, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with pd.read_json('data/quotes-2015.json.bz2', lines=True, compression='bz2', chunksize=100000) as df_reader:\n",
    "    for chunk in df_reader:\n",
    "        chunk_clean=chunk_filtering(chunk)\n",
    "        chunk_clean.to_csv(path_or_buf='data/clean_quotes-2015.bz2', compression='bz2', mode = 'a') # create a new csv files compress with bz2 containing all the dataframe recover from the chunk; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7620dc-490e-4311-b0cd-b188027133f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes_2015= pd.read_csv('data/clean_quotes-2016.bz2', compression='bz2')\n",
    "quotes_2015.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2ebf7f-c01e-443b-980a-354e326d4b9c",
   "metadata": {},
   "source": [
    "At result we obtained 5 set of Data each with a sufficient size, on total we obtained data to analyze."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52a6a56-c643-4f00-8e53-72611156d0f5",
   "metadata": {},
   "source": [
    "## Load additional data Relative to speakers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5adca85-ec46-4904-b3c5-0395e659767a",
   "metadata": {},
   "source": [
    "The provided speaker_attributes.parquet file contains attributes in terms of QIDs, thereby being uninterpretable by humans (df_qid).\n",
    "To map the QIDs to meaningful labels, we used the provied wikidata_labels_descriptions_quotebank.csv.bz2 containg the labels and value fo the respective QID containing the df_qid (df_label_qid)\n",
    "By combaning the information of both we can obtained usefule information about speakers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "280f0f07-1673-4e62-a70f-81296b7db81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qid = pd.read_parquet(\"speaker_attributes.parquet\",engine= \"pyarrow\" )\n",
    "dd_label_qid = pd.read_csv('data/wikidata_labels_descriptions_quotebank.csv.bz2', compression='bz2', index_col='QID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04552f3b-db5a-497c-a763-675961bc6370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aliases</th>\n",
       "      <th>date_of_birth</th>\n",
       "      <th>nationality</th>\n",
       "      <th>gender</th>\n",
       "      <th>lastrevid</th>\n",
       "      <th>ethnic_group</th>\n",
       "      <th>US_congress_bio_ID</th>\n",
       "      <th>occupation</th>\n",
       "      <th>party</th>\n",
       "      <th>academic_degree</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>candidacy</th>\n",
       "      <th>type</th>\n",
       "      <th>religion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Washington, President Washington, G. Washingt...</td>\n",
       "      <td>[+1732-02-22T00:00:00Z]</td>\n",
       "      <td>[Q161885, Q30]</td>\n",
       "      <td>[Q6581097]</td>\n",
       "      <td>1395141751</td>\n",
       "      <td>None</td>\n",
       "      <td>W000178</td>\n",
       "      <td>[Q82955, Q189290, Q131512, Q1734662, Q294126, ...</td>\n",
       "      <td>[Q327591]</td>\n",
       "      <td>None</td>\n",
       "      <td>Q23</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>[Q698073, Q697949]</td>\n",
       "      <td>item</td>\n",
       "      <td>[Q682443]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Douglas Noel Adams, Douglas Noël Adams, Dougl...</td>\n",
       "      <td>[+1952-03-11T00:00:00Z]</td>\n",
       "      <td>[Q145]</td>\n",
       "      <td>[Q6581097]</td>\n",
       "      <td>1395737157</td>\n",
       "      <td>[Q7994501]</td>\n",
       "      <td>None</td>\n",
       "      <td>[Q214917, Q28389, Q6625963, Q4853732, Q1884422...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Q42</td>\n",
       "      <td>Douglas Adams</td>\n",
       "      <td>None</td>\n",
       "      <td>item</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Paul Marie Ghislain Otlet, Paul Marie Otlet]</td>\n",
       "      <td>[+1868-08-23T00:00:00Z]</td>\n",
       "      <td>[Q31]</td>\n",
       "      <td>[Q6581097]</td>\n",
       "      <td>1380367296</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Q36180, Q40348, Q182436, Q1265807, Q205375, Q...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Q1868</td>\n",
       "      <td>Paul Otlet</td>\n",
       "      <td>None</td>\n",
       "      <td>item</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[George Walker Bush, Bush Jr., Dubya, GWB, Bus...</td>\n",
       "      <td>[+1946-07-06T00:00:00Z]</td>\n",
       "      <td>[Q30]</td>\n",
       "      <td>[Q6581097]</td>\n",
       "      <td>1395142029</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Q82955, Q15982858, Q18814623, Q1028181, Q1408...</td>\n",
       "      <td>[Q29468]</td>\n",
       "      <td>None</td>\n",
       "      <td>Q207</td>\n",
       "      <td>George W. Bush</td>\n",
       "      <td>[Q327959, Q464075, Q3586276, Q4450587]</td>\n",
       "      <td>item</td>\n",
       "      <td>[Q329646, Q682443, Q33203]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Velázquez, Diego Rodríguez de Silva y Velázqu...</td>\n",
       "      <td>[+1599-06-06T00:00:00Z]</td>\n",
       "      <td>[Q29]</td>\n",
       "      <td>[Q6581097]</td>\n",
       "      <td>1391704596</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Q1028181]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Q297</td>\n",
       "      <td>Diego Velázquez</td>\n",
       "      <td>None</td>\n",
       "      <td>item</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             aliases            date_of_birth  \\\n",
       "0  [Washington, President Washington, G. Washingt...  [+1732-02-22T00:00:00Z]   \n",
       "1  [Douglas Noel Adams, Douglas Noël Adams, Dougl...  [+1952-03-11T00:00:00Z]   \n",
       "2      [Paul Marie Ghislain Otlet, Paul Marie Otlet]  [+1868-08-23T00:00:00Z]   \n",
       "3  [George Walker Bush, Bush Jr., Dubya, GWB, Bus...  [+1946-07-06T00:00:00Z]   \n",
       "4  [Velázquez, Diego Rodríguez de Silva y Velázqu...  [+1599-06-06T00:00:00Z]   \n",
       "\n",
       "      nationality      gender   lastrevid ethnic_group US_congress_bio_ID  \\\n",
       "0  [Q161885, Q30]  [Q6581097]  1395141751         None            W000178   \n",
       "1          [Q145]  [Q6581097]  1395737157   [Q7994501]               None   \n",
       "2           [Q31]  [Q6581097]  1380367296         None               None   \n",
       "3           [Q30]  [Q6581097]  1395142029         None               None   \n",
       "4           [Q29]  [Q6581097]  1391704596         None               None   \n",
       "\n",
       "                                          occupation      party  \\\n",
       "0  [Q82955, Q189290, Q131512, Q1734662, Q294126, ...  [Q327591]   \n",
       "1  [Q214917, Q28389, Q6625963, Q4853732, Q1884422...       None   \n",
       "2  [Q36180, Q40348, Q182436, Q1265807, Q205375, Q...       None   \n",
       "3  [Q82955, Q15982858, Q18814623, Q1028181, Q1408...   [Q29468]   \n",
       "4                                         [Q1028181]       None   \n",
       "\n",
       "  academic_degree     id              label  \\\n",
       "0            None    Q23  George Washington   \n",
       "1            None    Q42      Douglas Adams   \n",
       "2            None  Q1868         Paul Otlet   \n",
       "3            None   Q207     George W. Bush   \n",
       "4            None   Q297    Diego Velázquez   \n",
       "\n",
       "                                candidacy  type                    religion  \n",
       "0                      [Q698073, Q697949]  item                   [Q682443]  \n",
       "1                                    None  item                        None  \n",
       "2                                    None  item                        None  \n",
       "3  [Q327959, Q464075, Q3586276, Q4450587]  item  [Q329646, Q682443, Q33203]  \n",
       "4                                    None  item                        None  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c6998e0-1136-4e9c-85ef-8ab6b28f8792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Q31</th>\n",
       "      <td>Belgium</td>\n",
       "      <td>country in western Europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q45</th>\n",
       "      <td>Portugal</td>\n",
       "      <td>country in southwestern Europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q75</th>\n",
       "      <td>Internet</td>\n",
       "      <td>global system of connected computer networks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q148</th>\n",
       "      <td>People's Republic of China</td>\n",
       "      <td>sovereign state in East Asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q155</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>country in South America</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Label                                   Description\n",
       "QID                                                                           \n",
       "Q31                      Belgium                     country in western Europe\n",
       "Q45                     Portugal                country in southwestern Europe\n",
       "Q75                     Internet  global system of connected computer networks\n",
       "Q148  People's Republic of China                  sovereign state in East Asia\n",
       "Q155                      Brazil                      country in South America"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00af751c-8c19-4283-9584-a6cd28b1cd2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Belgium'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc['Q31']['Label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8a1db0-542d-4b9f-a27b-c780e2a03267",
   "metadata": {},
   "source": [
    "# II- Filter the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1547a11-c37a-42a7-bc86-8abc812ef6e7",
   "metadata": {},
   "source": [
    "As a good data scientist, the first thing to do is to clean up the data . \n",
    "move the missing rows; if there are\n",
    "Let's check if the idenfier is unique, and we haven't duplicate rows;\n",
    "Let's check if the alias match the label for the df_qid data. \n",
    "\n",
    "\n",
    "We also need to extract quotation that refers to our subject of interest : climate change. \n",
    "To do so, we decided to creat a list of key_world (based of https://www.climaterealityproject.org/blog/key-terms-you-need-understand-climate-change) and extract contation containing these world. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d228cdc-96db-46a2-a49c-c29537b9986c",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_world = [\"carbon dioxide\", \"greenhouse gas\", \"global warming\",\n",
    "             \"climate change\",  \"fossil fuels\", \"sea-level rise\", \"renewable energy\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd7ccd0-27a9-4374-8edf-8365b0e2b876",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
