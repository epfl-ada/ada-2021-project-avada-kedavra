{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6630819-7845-4bfb-b746-6a7e780e0540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from urllib.request import urlopen\n",
    "import sqlite3\n",
    "\n",
    "#load the statistical libraries\n",
    "from statsmodels.stats import diagnostic\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1981b18-a20b-4b10-b585-0b15563a9d26",
   "metadata": {},
   "source": [
    "# General information Remark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d9c90b-4fd4-4d09-8163-7c2b7021c9b3",
   "metadata": {},
   "source": [
    "### In the loading part we will recover data from 2015 to 2020, however, first visulation (part III) will only be on the data from 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967b3b89-46e1-4377-96ab-830501273244",
   "metadata": {},
   "source": [
    "# I- Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edcfe19-c424-4435-86bd-b0eb11096498",
   "metadata": {},
   "source": [
    "### Load Quotebank data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6264bc75-8b74-44f3-9e45-c079a54bea3b",
   "metadata": {},
   "source": [
    "First, let's recover the quotation of interest : as project is based on the caracterisation of the speaker, we decide to pre-select the quotations that are related to a speaker (i.e speaker value is different from 'None'). \n",
    "Moreover, we select the quotations whose subject is related to climate change : to do so we create a list of key word based on https://www.climaterealityproject.org/blog/key-terms-you-need-understand-climate-change and select quotes that contains at least one of these word.  (cf chunk_filtering method) . We are aware that this methode incude biais,and we thought to later utlise NPL in order to filter quotation related to climate from other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab1771f-d38f-4add-a80e-769218de77d9",
   "metadata": {},
   "source": [
    "> ##### A/ Select data representative for climate interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36cf40e0-63d6-4ca6-a98e-f167ebcc24e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#declaration of a key_world list\n",
    "key_word = [\"carbon dioxide\", \"greenhouse gas\", \"global warming\",\n",
    "             \"climate change\",  \"fossil fuels\", \"sea-level rise\",\n",
    "             \"renewable energy\", \"CO2\",\"methane\",\"PPM\",\"COP\",\"GIEC\", \n",
    "             \"biofuels\",\"business as usual\", \"carbon footprint\", \"carbon neutral\", \"carbon sequestration\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b031381d-12de-49d0-beb2-11debbae3192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_filtering(chunk, lst):\n",
    "    template=[] #creation of an empty list :it's always cheaper to append to a list and create a DataFrame than append on a empty dataframe.\n",
    "    for i in lst: \n",
    "        template.append(chunk.loc[chunk[\"quotation\"].apply(lambda x : i in x) & \n",
    "                                  chunk[\"speaker\"].apply(lambda x: x!= \"None\")].drop(['phase'], axis=1))#select the quotation with value in speaker column different from 'None' \n",
    "                                                                                #and quotations containing the key word and drop Phase column\n",
    "        \n",
    "    return (pd.concat(template, ignore_index=True))# return a dataframe with our data of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd22b34b-ce66-41f4-82ba-a234b2327cba",
   "metadata": {},
   "source": [
    "##### *2020 quotes extractions*\n",
    " > The original dataset is of 792,3 Mo, so we decided to divide the dataset into chucks of 1000 rows and process each of them (by using the chunck_filtering). \n",
    "Then we load the process chunck into a new csv compressed bz2 file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28d4a71f-a51d-4a0b-a5c8-3fd15c6fa15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reader = pd.read_json('data/quotes-2020.json.bz2', lines=True, compression='bz2', chunksize=1000)\n",
    "for i, chunk in enumerate(df_reader):\n",
    "        chunk_clean=chunk_filtering(chunk, key_word) #recover interested row of the chunk\n",
    "        header = i == 0 #we kept the name of the column only for the first chunk\n",
    "        mode = 'w' if i == 0 else 'a' # For appending data to an existing CSV file (so for every chunk exepct the first one), \n",
    "                                        #we can use mode = a\n",
    "            \n",
    "        chunk_clean.to_csv(path_or_buf=\"data/clean_quotes-2020.bz2\",compression='bz2',header=header, mode=mode, index = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9edd5dc6-4e47-4744-94cc-5d0428f06a49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "quotes_2020= pd.read_csv('data/clean_quotes-2020.bz2', compression='bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f4e2620-72c8-4778-8767-4d6f46facabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " We extract 11091 quotes from the 2020 files\n"
     ]
    }
   ],
   "source": [
    "print( \" We extract {} quotes from the 2020 files\".format(len(quotes_2020)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae309716-2b59-4d84-8cac-259f2b256a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quoteID</th>\n",
       "      <th>quotation</th>\n",
       "      <th>speaker</th>\n",
       "      <th>qids</th>\n",
       "      <th>date</th>\n",
       "      <th>numOccurrences</th>\n",
       "      <th>probas</th>\n",
       "      <th>urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-02-24-013709</td>\n",
       "      <td>For every doubling of carbon dioxide concentra...</td>\n",
       "      <td>E. Calvin Beisner</td>\n",
       "      <td>['Q19877395']</td>\n",
       "      <td>2020-02-24 16:02:23</td>\n",
       "      <td>1</td>\n",
       "      <td>[['E. Calvin Beisner', '0.677'], ['None', '0.3...</td>\n",
       "      <td>['https://www.heartland.org/news-opinion/news/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-02-24-028340</td>\n",
       "      <td>If you're a doctor that cares about the wellbe...</td>\n",
       "      <td>Fiona Stanley</td>\n",
       "      <td>['Q1653736']</td>\n",
       "      <td>2020-02-24 12:45:00</td>\n",
       "      <td>4</td>\n",
       "      <td>[['Fiona Stanley', '0.9473'], ['None', '0.0527']]</td>\n",
       "      <td>['http://watoday.com.au/business/banking-and-f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             quoteID                                          quotation  \\\n",
       "0  2020-02-24-013709  For every doubling of carbon dioxide concentra...   \n",
       "1  2020-02-24-028340  If you're a doctor that cares about the wellbe...   \n",
       "\n",
       "             speaker           qids                 date  numOccurrences  \\\n",
       "0  E. Calvin Beisner  ['Q19877395']  2020-02-24 16:02:23               1   \n",
       "1      Fiona Stanley   ['Q1653736']  2020-02-24 12:45:00               4   \n",
       "\n",
       "                                              probas  \\\n",
       "0  [['E. Calvin Beisner', '0.677'], ['None', '0.3...   \n",
       "1  [['Fiona Stanley', '0.9473'], ['None', '0.0527']]   \n",
       "\n",
       "                                                urls  \n",
       "0  ['https://www.heartland.org/news-opinion/news/...  \n",
       "1  ['http://watoday.com.au/business/banking-and-f...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes_2020.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea50135-f229-42c6-91ce-52b28ec43bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ae4e97a-4d31-4c3f-a9ce-0c710073745d",
   "metadata": {},
   "source": [
    "##### *2019 quotes extractions*\n",
    "> The original dataset is of 3.32 Go, so we decided to divide the dataset into chucks of 1000 rows and process each of them (by using the chunck_filtering). Then we load the process chunck into a new csv compressed bz2 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d516035-0364-44b3-b0c0-89a39d738e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reader = pd.read_json('data/quotes-2019.json.bz2', lines=True, compression='bz2', chunksize=1000)\n",
    "for i, chunk in enumerate(df_reader):\n",
    "        chunk_clean=chunk_filtering(chunk, key_word) #recover interested row of the chunk\n",
    "        header = i == 0 #we kept the name of the column only for the first chunk\n",
    "        mode = 'w' if i == 0 else 'a' # For appending data to an existing CSV file (so for every chunk exepct the first one), \n",
    "                                        #we can use mode = a\n",
    "            \n",
    "        chunk_clean.to_csv(path_or_buf=\"data/clean_quotes-2019.bz2\",compression='bz2',header=header, mode=mode, index = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4947d82f-97dd-4d82-95ed-cd580d10f6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes_2019= pd.read_csv('data/clean_quotes-2019.bz2', compression='bz2') # load into the quotes_2019 df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93186176-a9be-4635-a87b-35816d8a7f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47280, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print( \" We extracted {} quotes from the 2019 files\".format(len(quotes_2019)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898e8ab3-00a4-4dc9-b89f-e3958ed2bc84",
   "metadata": {},
   "source": [
    "##### *2018 quotes extractions*\n",
    "> The original dataset is of 4.48 Go, so we decided to divide the dataset into chucks of 1000 rows and process each of them ((by using the chunck_filtering). Then we load the process chunck into a new csv compressed bz2 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84914c33-8155-49ac-8d6a-d06d3f904e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reader = pd.read_json('data/quotes-2018.json.bz2', lines=True, compression='bz2', chunksize=1000)\n",
    "for i, chunk in enumerate(df_reader):\n",
    "        chunk_clean=chunk_filtering(chunk, key_word) #recover interested row of the chunk\n",
    "        header = i == 0 #we kept the name of the column only for the first chunk\n",
    "        mode = 'w' if i == 0 else 'a' # For appending data to an existing CSV file (so for every chunk exepct the first one), \n",
    "                                        #we can use mode = a\n",
    "            \n",
    "        chunk_clean.to_csv(path_or_buf=\"data/clean_quotes-2018.bz2\",compression='bz2',header=header, mode=mode, index = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca002f8c-56f4-4940-b198-a214ce2951f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes_2018= pd.read_csv('data/clean_quotes-2018.bz2', compression='bz2') #load the data to quotes_2018 df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f4203a5-49de-42f8-9168-6e97ef3b2602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " We extracted 35847 quotes from the 2018 files\n"
     ]
    }
   ],
   "source": [
    "print( \" We extracted {} quotes from the 2018 files\".format(len(quotes_2018)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8650499-8b64-4279-8cec-22c294b1f67c",
   "metadata": {},
   "source": [
    "##### *2017 quotes extractions*\n",
    "> The original dataset is of 4.84 Go, so we decided to divide the dataset into chucks of 1000 rows and process each of them (by using the chunck_filtering). Then we load the process chunck into a new csv compressed bz2 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0c3dec8-5fa1-474c-9141-ea4e2fc85608",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reader = pd.read_json('data/quotes-2017.json.bz2', lines=True, compression='bz2', chunksize=1000)\n",
    "for i, chunk in enumerate(df_reader):\n",
    "        chunk_clean=chunk_filtering(chunk, key_word) #recover interested row of the chunk\n",
    "        header = i == 0 #we kept the name of the column only for the first chunk\n",
    "        mode = 'w' if i == 0 else 'a' # For appending data to an existing CSV file (so for every chunk exepct the first one), \n",
    "                                        #we can use mode = a\n",
    "            \n",
    "        chunk_clean.to_csv(path_or_buf=\"data/clean_quotes-2017.bz2\",compression='bz2',header=header, mode=mode, index = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fc42032-a525-4e90-9247-3a2e591aa26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes_2017= pd.read_csv('data/clean_quotes-2017.bz2', compression='bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c46c386-4866-431c-8fea-20707695e187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " We extracted 35324 quotes from the 2017 files\n"
     ]
    }
   ],
   "source": [
    "print( \" We extracted {} quotes from the 2017 files\".format(len(quotes_2017)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3274154-6ca0-4d95-a254-92ee85f4fd9a",
   "metadata": {},
   "source": [
    "##### *2016 quotes extractions*\n",
    " > The original dataset is of 2.16 Go, so we decided to divide the dataset into chucks of 1000 rows and process each of them(by using the chunck_filtering). Then we load the process chunck into a new csv compressed bz2 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7dec2729-c0b5-419a-be2a-e39df7639e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reader = pd.read_json('data/quotes-2016.json.bz2', lines=True, compression='bz2', chunksize=1000)\n",
    "for i, chunk in enumerate(df_reader):\n",
    "        chunk_clean=chunk_filtering(chunk, key_word) #recover interested row of the chunk\n",
    "        header = i == 0 #we kept the name of the column only for the first chunk\n",
    "        mode = 'w' if i == 0 else 'a' # For appending data to an existing CSV file (so for every chunk exepct the first one), \n",
    "                                        #we can use mode = a\n",
    "            \n",
    "        chunk_clean.to_csv(path_or_buf=\"data/clean_quotes-2016.bz2\",compression='bz2',header=header, mode=mode, index = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31fc23d3-9713-4206-a126-54882bc6fc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes_2016= pd.read_csv('data/clean_quotes-2016.bz2', compression='bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "418d9ef6-3d36-42a4-a7ee-845d030cba7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " We extracted 18344 quotes from the 2016 files\n"
     ]
    }
   ],
   "source": [
    "print( \" We extracted {} quotes from the 2016 files\".format(len(quotes_2016)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf4b6c0-579e-49e4-ae1f-8096fdbf0821",
   "metadata": {},
   "source": [
    "##### *2015 quotes extractions*\n",
    "> The original dataset is of 3.11 Go, so we decided to divide the dataset into chucks of 1000 rows and process each of them(by using the chunck_filtering). Then we load the process chunck into a new csv compressed bz2 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8131f70-68ac-4126-a31d-f37d8944f444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13829, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reader = pd.read_json('data/quotes-2015.json.bz2', lines=True, compression='bz2', chunksize=1000)\n",
    "for i, chunk in enumerate(df_reader):\n",
    "        chunk_clean=chunk_filtering(chunk, key_word) #recover interested row of the chunk\n",
    "        header = i == 0 #we kept the name of the column only for the first chunk\n",
    "        mode = 'w' if i == 0 else 'a' # For appending data to an existing CSV file (so for every chunk exepct the first one), \n",
    "                                        #we can use mode = a\n",
    "            \n",
    "        chunk_clean.to_csv(path_or_buf=\"data/clean_quotes-2015.bz2\",compression='bz2',header=header, mode=mode, index = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba7620dc-490e-4311-b0cd-b188027133f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "quotes_2015= pd.read_csv('data/clean_quotes-2015.bz2', compression='bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9150b06c-7c29-44e2-94e3-cecb975ac80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " We extracted 35176 quotes from the 2015 files\n"
     ]
    }
   ],
   "source": [
    "print( \" We extracted {} quotes from the 2015 files\".format(len(quotes_2015)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "12781cd5-f83d-4297-b58a-1e06a212bca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" At result, we extracted {} quotes fromes quotebank data\".format((len(quotes_2015)+len(quotes_2016)+len(quotes_2017)\n",
    "                                                                         +len(quotes_2018)+len(quotes_2019)+len(quotes_2020)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2ebf7f-c01e-443b-980a-354e326d4b9c",
   "metadata": {},
   "source": [
    "Even with key_word selection we success to extrat interesting data from the Quotebank data with a sufficient size. Let's add another dataset that will give us characteristic information about the speaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bae988-491b-4a10-bf08-0f1e6e076c22",
   "metadata": {},
   "source": [
    "> ##### B/ Select data representative for climate septic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4572fa-ffe1-4abc-8bc0-f911d039ad5c",
   "metadata": {},
   "source": [
    "We want to asses climate scepticism among our speakers. We selected 10 speakers that are said to be climate sceptic according to https://www.businessinsider.com/the-ten-most-important-climate-change-skeptics-2009-7?IR=T#dont-miss-11. We want to find our list of keywords from their quotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dde1b1e3-28b0-49e3-a1ca-9723b945ecea",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = ['Freeman Dyson', 'Bjorn Lomborg', 'Myron Ebell', 'Kiminori Itoh', 'Ivar Giaever', \n",
    "       'Will Happer', 'Ian Plimer', 'Michael Chrichton', 'Alan Carlin', 'Patrick Michaels'] #list of the name taken from the article\n",
    "#iteration in the list of name in order to find if our people of interest are in our quotes list and \n",
    "#we then create one df per year with their correspondings quotes\n",
    "\n",
    "template = []\n",
    "\n",
    "for i in lst:\n",
    "      template.append(quotes_2020.loc[quotes_2020['speaker'].apply(lambda x : i == x)])  \n",
    "        \n",
    "df_2020 = pd.concat(template, ignore_index=True)\n",
    "\n",
    "template = []\n",
    "\n",
    "for i in lst:\n",
    "      template.append(quotes_2019.loc[quotes_2019['speaker'].apply(lambda x : i == x)])  \n",
    "        \n",
    "df_2019 = pd.concat(template, ignore_index=True)\n",
    "\n",
    "template = []\n",
    "\n",
    "for i in lst:\n",
    "      template.append(quotes_2018.loc[quotes_2018['speaker'].apply(lambda x : i == x)])  \n",
    "        \n",
    "df_2018 = pd.concat(template, ignore_index=True)\n",
    "\n",
    "template = []\n",
    "\n",
    "for i in lst:\n",
    "      template.append(quotes_2017.loc[quotes_2017['speaker'].apply(lambda x : i == x)])  \n",
    "        \n",
    "df_2017 = pd.concat(template, ignore_index=True)\n",
    "\n",
    "template = []\n",
    "\n",
    "for i in lst:\n",
    "      template.append(quotes_2016.loc[quotes_2016['speaker'].apply(lambda x : i == x)])  \n",
    "        \n",
    "df_2016 = pd.concat(template, ignore_index=True)\n",
    "\n",
    "template = []\n",
    "\n",
    "for i in lst:\n",
    "      template.append(quotes_2015.loc[quotes_2015['speaker'].apply(lambda x : i == x)])  \n",
    "        \n",
    "df_2015 = pd.concat(template, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27423a0e-3826-4503-8dfe-81c3326f4625",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will now construct a list with the quotations only\n",
    "quotations_2020 = df_2020['quotation'].tolist()\n",
    "quotations_2019 = df_2019['quotation'].tolist()\n",
    "quotations_2018 = df_2018['quotation'].tolist()\n",
    "quotations_2017 = df_2017['quotation'].tolist()\n",
    "quotations_2016 = df_2016['quotation'].tolist()\n",
    "quotations_2015 = df_2015['quotation'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "281bc0e0-c082-413f-9fc9-0e6f3c02e370",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/maria/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/maria/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#we imported these librairies in order to handle language expression and word counting \n",
    "import nltk\n",
    "import nltk.corpus\n",
    "from nltk.tokenize import word_tokenize \n",
    "nltk.download('punkt')\n",
    "from nltk.probability import FreqDist\n",
    "nltk.download('words')\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1fd5e9f0-3951-46a8-838f-34f4548f150c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#two functions\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "a = set(stopwords.words('english'))\n",
    "\n",
    "def remov_punc(lst): #removes the punctuations from a sentence\n",
    "    punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_°~''' #list of punctuations \n",
    "    remov_punc = []\n",
    "    t = 0\n",
    "    \n",
    "    for i in lst :\n",
    "        t=t+1\n",
    "        for d in i:\n",
    "            if d in punc:\n",
    "                i = i.replace(d, \" \")\n",
    "        remov_punc.append(i)\n",
    "    return remov_punc\n",
    "    \n",
    "def words_freq(lst): #calculate each word frequency\n",
    "    ls=[]\n",
    "    for i in lst: \n",
    "        text = i\n",
    "        text1 = word_tokenize(text.lower())\n",
    "        imp_words = [x for x in text1 if x not in a]\n",
    "        ls.append(imp_words)\n",
    "    return ls\n",
    "\n",
    "def words__highest_freq(lst): #return the highest word frequency\n",
    "    ls_freq = []\n",
    "    for i in lst: \n",
    "        fdist = FreqDist(i)\n",
    "        fdist1 = fdist.most_common(1)\n",
    "        ls_freq.append(fdist1)\n",
    "    return ls_freq #this is a list with the highest frequency for each most written words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78c5a5d9-8fcf-48dd-8c69-f7b2dc2aec91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['science',\n",
       " 'increasing',\n",
       " 'local',\n",
       " 'demonstration',\n",
       " 'climate',\n",
       " 'energy',\n",
       " 'emissions',\n",
       " 'power',\n",
       " 'degrees',\n",
       " 'percent',\n",
       " 'c',\n",
       " 'silly',\n",
       " 'paris',\n",
       " 'co2',\n",
       " 'consensus',\n",
       " 'global',\n",
       " 'effects',\n",
       " 'models',\n",
       " 'r',\n",
       " 'ipcc',\n",
       " 'years',\n",
       " 'year']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_freq_2020 = words_freq(remov_punc(quotations_2020))\n",
    "words_freq_2019 = words_freq(remov_punc(quotations_2019))\n",
    "words_freq_2018 = words_freq(remov_punc(quotations_2018))\n",
    "words_freq_2017 = words_freq(remov_punc(quotations_2017))\n",
    "words_freq_2016 = words_freq(remov_punc(quotations_2016))\n",
    "words_freq_2015 = words_freq(remov_punc(quotations_2015))\n",
    "\n",
    "w_freq = words_freq_2020 + words_freq_2019 + words_freq_2018 + words_freq_2017 + words_freq_2016 + words_freq_2015\n",
    "w_h_freq = words__highest_freq(w_freq)\n",
    "w_h_freq\n",
    "\n",
    "keywords_sceptic = []\n",
    "\n",
    "for i in w_h_freq:\n",
    "    for d in i: \n",
    "        if d[1] >=3 : \n",
    "            if d[0] not in keywords_sceptic:\n",
    "                keywords_sceptic.append(d[0])\n",
    "        \n",
    "keywords_sceptic #our list of keywords according to their representation in the climate sceptic speaker quotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a8788628-8a0b-4800-aeb7-f456beb3ca37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on 2020\n",
    "df_reader = pd.read_json('data/quotes-2020.json.bz2', lines=True, compression='bz2', chunksize=1000)\n",
    "for i, chunk in enumerate(df_reader):\n",
    "        chunk_clean=chunk_filtering(chunk, keywords_sceptic  ) #recover interested row of the chunk\n",
    "        header = i == 0 #we kept the name of the column only for the first chunk\n",
    "        mode = 'w' if i == 0 else 'a' # For appending data to an existing CSV file (so for every chunk exepct the first one), \n",
    "                                        #we can use mode = a\n",
    "            \n",
    "        chunk_clean.to_csv(path_or_buf=\"data/clean_quotes_sceptic-2020.bz2\",compression='bz2',header=header, mode=mode, index = False )\n",
    "#6 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3af73b58-8ff4-4a2c-9007-85c0319832b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes_2020_sceptic= pd.read_csv('data/clean_quotes_sceptic-2020.bz2', compression='bz2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52a6a56-c643-4f00-8e53-72611156d0f5",
   "metadata": {},
   "source": [
    "## Load additional data Relative to speakers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5adca85-ec46-4904-b3c5-0395e659767a",
   "metadata": {},
   "source": [
    "The provided speaker_attributes.parquet file contains attributes in terms of QIDs, thereby being uninterpretable by humans (df_qid).\n",
    "To map the QIDs to meaningful labels, we used the provied wikidata_labels_descriptions_quotebank.csv.bz2 containg the labels and value fo the respective QID containing the df_qid (df_label_qid)\n",
    "By combaning the information of both we can obtained usefule information about speakers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "280f0f07-1673-4e62-a70f-81296b7db81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qid = pd.read_parquet(\"speaker_attributes.parquet\",engine= \"pyarrow\" )\n",
    "df_label_qid = pd.read_csv('data/wikidata_labels_descriptions_quotebank.csv.bz2', compression='bz2', index_col='QID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2db133-e8c3-43aa-ae20-07ded9ad40a7",
   "metadata": {},
   "source": [
    "Before extract the label of qid, let's check which column we want to keep in frame with our project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b98c8e-74a1-44c5-bc6e-395b22c4440b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_qid.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a524d4e3-29a4-49eb-b646-c6fc63e54220",
   "metadata": {},
   "source": [
    "Let's verify that academic_degree has revelant values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "293730b9-06ea-4bfd-909e-ab8a7cb806a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There's no academic degree revelant value ? False\n"
     ]
    }
   ],
   "source": [
    "#print(\"There's no academic degree revelant value ? {}\".format(all(df_qid.academic_degree.isna())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573efeb6-dd9b-4121-8e10-d926f222795d",
   "metadata": {},
   "source": [
    "We decided to drop lastrevid, US_congress_bio_ID, type. Moreover, it's seems that academic_degree value are rare, let's check that too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "677d65e5-50ff-480a-b5ae-6f7d18551117",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qid.drop(['lastrevid', 'US_congress_bio_ID', 'type'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6b7ee1f-81ec-4b1d-b038-7e89c44d3943",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We found out that some of the QIDs used in the speaker attribute file are actually redirection from an original QID. \n",
    "#We will manulally add their corresponding information using the orginal QID. We found the corespondance manualy between the two. \n",
    "#Here, there are in order, respectively the redirection QID, and its corresponding original one. One of he QID was only present \n",
    "#as a redirection, so we manually added this one (Q3186984), and its corresponding info. \n",
    "\n",
    "redirect_QID=['Q3268166', 'Q11815360', 'Q12014399', 'Q16287483',\n",
    "              'Q20432251', 'Q21550646', 'Q13365117', 'Q13424794',\n",
    "             'Q1248362', 'Q6859927', 'Q15145782',\n",
    "             'Q15991263', 'Q12455619', 'Q5568256', \n",
    "             'Q6363085', 'Q11819457', 'Q12334852', 'Q15145783']\n",
    "actual_QID=['Q1113899', 'Q1919436', 'Q250867', 'Q6051619',\n",
    "             'Q26934816', 'Q18431816', 'Q12840545', 'Q5157338',\n",
    "            'Q3455803', 'Q715222', 'Q1052281',\n",
    "            'Q2743689', 'Q7019111', 'Q3738699', \n",
    "            'Q380075', 'Q3391743', 'Q476246', 'Q2449503']\n",
    "\n",
    "#There is a QID that was deleted from Wikidata, Q99753484, so we will remove this QID later \n",
    "\n",
    "lst=[['Journalist', 'monthly magazine of the United Kingdom‘s National Union of Journalists (NUJ)']]\n",
    "indexes=['Q3186984']\n",
    "col=['Label', 'Description']\n",
    "for i in range(len(redirect_QID)):\n",
    "    lst.append([df_label_qid.loc[actual_QID[i]]['Label'], \n",
    "                df_label_qid.loc[actual_QID[i]]['Description']])\n",
    "    indexes.append(redirect_QID[i])\n",
    "\n",
    "additional_df= pd.DataFrame(lst, columns= col, index=indexes)\n",
    "df_label_qid_co=df_label_qid.append(additional_df, ignore_index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a97b9e-3b81-4c2f-a117-5af59e5bb691",
   "metadata": {},
   "source": [
    "As this function make several minute (more than 20 min) to run, we decided to create a compressed csv files in order to run these cells once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd7bb80b-736d-4e85-9e96-0d913a335b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_column (df_climate, df_septic) : \n",
    "    for i, skr in enumerate(df_qid.label) : \n",
    "        if (df_climate['speaker'].isin([skr]).any() & df_septic['speaker'].isin([skr]).any()) : \n",
    "            df_qid.loc[i, 'climate']='None'\n",
    "        else if (df_climate['speaker'].isin([skr]).any()) :\n",
    "            df_qid.loc[i, 'climate']='climate'\n",
    "        else if (df_septic['speaker'].isin([skr]).any()) : \n",
    "            df_qid.loc[i, 'climate']='climate_septic'\n",
    "        else : df_qid.loc[i, 'climate']='None'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17875fab-7cbe-44b9-b911-2c7298b827e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that loops through a column to replace QID with their label, and skip None values. We will deal with them later, \n",
    "#when we will use the different data.\n",
    "def extraction_label(df) : \n",
    "    liste=[]\n",
    "    for row  in df: \n",
    "        if row is None: \n",
    "            continue #skip None values\n",
    "        template=[]\n",
    "        for value in row: #iterating over the values of a cell, as there are multiple QIDs in some of them.\n",
    "            if value == 'Q99753484': #To filter the deleted QID\n",
    "                continue\n",
    "            template.append(df_label_qid_co.loc[value]['Label']) #Map the QID to its corresponding label. \n",
    "        liste.append(template)    \n",
    "    return pd.Series(liste)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fea4a52b-ac98-496c-bd3d-b6a238f46a16",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/dp/xm3y4vf11sd5mzn0yd618jhm0000gn/T/ipykernel_13175/1015064285.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Applying the function to every column containing QIDs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf_qid\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nationality'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gender'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ethnic_group'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'occupation'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'party'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'academic_degree'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'candidacy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'religion'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_qid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nationality'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gender'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ethnic_group'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'occupation'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'party'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'academic_degree'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'candidacy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'religion'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextraction_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0madd_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquotes_2020\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquotes_2020_septic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf_qid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"data/speaker_attribute.bz2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'bz2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/dp/xm3y4vf11sd5mzn0yd618jhm0000gn/T/ipykernel_13175/4168448170.py\u001b[0m in \u001b[0;36madd_column\u001b[0;34m(df_climate, df_septic)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0madd_column\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf_climate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_septic\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_qid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf_climate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'speaker'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mskr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mdf_septic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'speaker'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mskr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m             \u001b[0mdf_qid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'climate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'None'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf_climate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'speaker'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mskr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ada/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36misin\u001b[0;34m(self, values)\u001b[0m\n\u001b[1;32m   5023\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5024\u001b[0m         \"\"\"\n\u001b[0;32m-> 5025\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5026\u001b[0m         return self._constructor(result, index=self.index).__finalize__(\n\u001b[1;32m   5027\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"isin\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ada/lib/python3.8/site-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36misin\u001b[0;34m(comps, values)\u001b[0m\n\u001b[1;32m    522\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismember\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Applying the function to every column containing QIDs. \n",
    "df_qid [['nationality', 'gender', 'ethnic_group','occupation', 'party', 'academic_degree', 'candidacy', 'religion']] = df_qid[['nationality', 'gender', 'ethnic_group','occupation', 'party', 'academic_degree', 'candidacy', 'religion']].apply(extraction_label)\n",
    "add_column(quotes_2020, quotes_2020_septic)\n",
    "df_qid.to_csv(path_or_buf=\"data/speaker_attribute.bz2\", compression = 'bz2', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27445a9-1291-43ef-82a2-d1c93a6799a6",
   "metadata": {},
   "source": [
    "## Load data about natural disasters and important political event\n",
    "\n",
    "We would like to compare how climate change speech change in media relatively to the important events occuring in the world. To do so we would like to have a list of important natural disasters that have occured between 2015 and 2020 as well as a list of political and diplomatic events more or less related to climate.\n",
    "\n",
    "The final goal would be the create these lists from Wikipedia scraping data. For now, we made two non-exhaustive \"handmade\" ones :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "826fd3a2-8d58-4c09-8715-6aec1b1f7996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Name        Date\n",
      "0                           Earthquake in Nepal   2015-4-25\n",
      "1                           Earthquake in Nepal   2015-5-12\n",
      "2               Heat waves in Inda and Pakistan    2015-4-1\n",
      "3                                Flood in India   2015-11-1\n",
      "4   Typhoon inundate Myanmar, Bangladesh, India    2015-7-1\n",
      "5       Massive floods in Malawi and Mozambique   2015-3-14\n",
      "6                           Drought in Ethiopia    2015-2-1\n",
      "7                            Hurricane in Haiti   2016-10-4\n",
      "8                         Earthquake in Ecuador   2016-4-16\n",
      "9                      Hurricane in Puerto Rico   2017-9-16\n",
      "10                   Hurricane season in the US    2016-6-1\n",
      "11                  Amazon rainforest wildfires    2019-6-1\n",
      "12                           Fires in Australia    2019-6-1\n",
      "13                            Covid-19 pandemic  2019-11-16\n",
      "14                              Floods in Nepal   2020-7-11\n",
      "15                              Fires in the US   2020-7-24\n",
      "                                                Name        Date\n",
      "0                                              COP21  2015-12-12\n",
      "1                                     Trump Election   2016-11-8\n",
      "2        Trump announce quitting \"l'Accord de Paris\"    2017-6-1\n",
      "3                    CLimate strikes begun in Sweden   2018-8-20\n",
      "4    Official letter for quitting\"l'Accord de Paris\"   2019-11-4\n",
      "5  Official retreat of the US from \"l'Accord de P...   2020-11-4\n",
      "6                                     Biden Election   2020-11-3\n"
     ]
    }
   ],
   "source": [
    "data = {'Name':['Earthquake in Nepal','Earthquake in Nepal','Heat waves in Inda and Pakistan','Flood in India','Typhoon inundate Myanmar, Bangladesh, India','Massive floods in Malawi and Mozambique','Drought in Ethiopia','Hurricane in Haiti','Earthquake in Ecuador','Hurricane in Puerto Rico','Hurricane season in the US','Amazon rainforest wildfires','Fires in Australia','Covid-19 pandemic','Floods in Nepal','Fires in the US'],'Date':['2015-4-25','2015-5-12','2015-4-1','2015-11-1','2015-7-1','2015-3-14','2015-2-1','2016-10-4','2016-4-16','2017-9-16','2016-6-1','2019-6-1','2019-6-1','2019-11-16','2020-7-11','2020-7-24']}\n",
    "natural_disasters = pd.DataFrame(data)\n",
    "print(natural_disasters)\n",
    "\n",
    "data = {'Name':['COP21','Trump Election','Trump announce quitting \\\"l\\'Accord de Paris\\\"','CLimate strikes begun in Sweden','Official letter for quitting\\\"l\\'Accord de Paris\\\"','Official retreat of the US from \\\"l\\'Accord de Paris\\\"','Biden Election'],'Date':['2015-12-12','2016-11-8','2017-6-1','2018-8-20','2019-11-4','2020-11-4','2020-11-3']}\n",
    "political_events = pd.DataFrame(data)\n",
    "print(political_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8a1db0-542d-4b9f-a27b-c780e2a03267",
   "metadata": {
    "tags": []
   },
   "source": [
    "# II- Filter the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1547a11-c37a-42a7-bc86-8abc812ef6e7",
   "metadata": {},
   "source": [
    "As a good data scientist, the first thing to do is to clean up the data : we need to filtered missing and duplicates rows if there are presented. We will only filter data from Quotebank extraction as we speakers_file is only ??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e534141a-105f-48ba-9e87-daa98425ca57",
   "metadata": {},
   "source": [
    "> ##### *check for missing row*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a824dab5-3740-49b3-b29e-0f0b401546a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is there some missing rows ? False \n"
     ]
    }
   ],
   "source": [
    "print(\"Is there some missing rows ? {} \".format(np.array([quotes_2020.isnull().any(axis=1)]).all()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d751443c-4cb1-4ee9-9866-aaee5fde6d4c",
   "metadata": {},
   "source": [
    "> ##### *check for duplicate* \n",
    "We define a function that receive a dataframe (quotes_2020 ... quotes_2015) and remove their duplicates rows according to duplicate quotation if the speakers and the date is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28e24b9c-41ef-4760-b305-4def17cb6faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicates (df): \n",
    "    \n",
    "    if df[\"quotation\"].is_unique  == False & df[\"speaker\"].is_unique == False & df[\"date\"].is_unique == False: \n",
    "        df.drop_duplicates(['quotation'], keep='first', inplace=True) #remove the duplicate rows directly on the df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b75326db-260f-4ffa-ac6f-abcf76c54eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We still get 10486 quotes from the 2020 dataset\n"
     ]
    }
   ],
   "source": [
    "check_duplicates(quotes_2020)\n",
    "#check_duplicates(quotes_2019)\n",
    "#check_duplicates(quotes_2018)\n",
    "#check_duplicates(quotes_2017)\n",
    "#check_duplicates(quotes_2016)\n",
    "#check_duplicates(quotes_2017)\n",
    "#check_duplicates(quotes_2015)\n",
    "print( \"We still get {} quotes from the 2020 dataset\".format(len(quotes_2020)))\n",
    "#print( \"We still get {} quotes from the 2020 dataset\".format(len(quotes_2019)))\n",
    "#print( \"We still get {} quotes from the 2020 dataset\".format(len(quotes_2018)))\n",
    "#print( \"We still get {} quotes from the 2020 dataset\".format(len(quotes_2017)))\n",
    "#print( \"We still get {} quotes from the 2020 dataset\".format(len(quotes_2016)))\n",
    "#print( \"We still get {} quotes from the 2020 dataset\".format(len(quotes_2015)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe810cb-20ae-4641-8739-39b7bb7511c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creation of a dataframe containing speakers_attribute and there interest for the climat "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175c5ad4-75a4-4dc2-a238-69389de26f45",
   "metadata": {},
   "source": [
    "As a primary analysis, we decided to only look at speaker cited from quotation (so interest to climate problematic) and seek if caracterstic might contribute to their interest. To do so, we simply decide to create a new column to the speakers dataframe which value is one if the label value correspond to a speakers in our defined quotes dataframe or 0 if not.\n",
    "> NB : we will only use the speakers extrated from 2020 for our first analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd50cb19-3e40-4d93-99f4-9faa357b9ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's check if the label is unique\n",
    "speakers.label.is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d777ef-b111-4165-8b90-c6f70b0822cb",
   "metadata": {},
   "source": [
    "We can observe that  'label' from the speaker dataframe  is non unique meaning that they are multiple personne with the same name that have different caracteristics. This cause us trouble cause we can't propely indentify one quotation with a unique personne with singular caracteristic, but instead one quotation can be attribute to different personne with different caracteristics. \n",
    "For this primary task we decide to non discrimate the label and attribute them the same climate interest (i.e if a speaker is named x in the quotes data, then all label named x in the speakers df will be considere as interest for climate change). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9621b883-9ef3-401d-9062-80cb73d482e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4a8f58c-5c08-44d5-81fc-151fdec345a0",
   "metadata": {},
   "source": [
    "# III-Exploration of our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5271ef-c0c5-480c-adb8-f79a0d822615",
   "metadata": {},
   "source": [
    "Let's see some distribution and statitics: \n",
    " - aged people vs yound people \n",
    " - party politics  \n",
    " - number of quotations in function of natural disaster and important political event \n",
    "ect... \n",
    "\n",
    "stat : correlation coeff ; m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5756638-5cba-49a1-82a2-9c06b7363eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "climate['age'].hist(bins = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c75937c-9a60-49aa-a30f-01c20162181a",
   "metadata": {},
   "outputs": [],
   "source": [
    "climate['age'].hist(bins = 50).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1415150f-96b0-4c26-b118-53430195e60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#does data comes from normal distribution ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db72f330-1313-4e6d-ba37-9a9e13f262df",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic.kstest_normal(climate['age'].values, dist = 'norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f9ce94-1a6b-4e4e-b573-b1519ec14bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#does data comes from exponential distribution ?\n",
    "#how about exponential?\n",
    "diagnostic.kstest_normal(climate['age'].values, dist = 'exp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013eec79-8488-4ec1-afe3-c4c36683b4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#is party politics is correlated to climate preocupation ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66fc7a8-b7b6-4219-a46b-be40b7726d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.pearsonr(df['IncomePerCap'],df['Employed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee35e9d4-2475-4329-bdae-ffa80feff36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(lalonde_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e822d2b-4d7d-4f6b-8aab-399f8990117a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc8cb77a-4512-4c34-8881-fac754d1c9b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# IV-Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd0a250-afc8-4c34-9539-7f3bfecf9b80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
