{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6630819-7845-4bfb-b746-6a7e780e0540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from urllib.request import urlopen\n",
    "import sqlite3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967b3b89-46e1-4377-96ab-830501273244",
   "metadata": {},
   "source": [
    "# I- Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8fff96-1cf0-4d89-84d8-d2e0d780c352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question : garder  le phase quand quotation ? index à retirer ? si oui :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2274bf8-bcfc-4df4-aba5-f737aa52cf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_filtering(chunk, lst):\n",
    "    template=[] #creation of an empty list :it's always cheaper to append to a list and create a DataFrame than append on a empty dataframe.\n",
    "    for i in lst: \n",
    "        template.append(chunk.loc[chunk[\"quotation\"].apply(lambda x : i in x) & \n",
    "                                  chunk[\"speaker\"].apply(lambda x: x!= \"None\")].drop(['phase'], axis=1)#select the quotation with value in speaker column different from 'None' \n",
    "                                                                                #and quotations containing the key word\n",
    "        \n",
    "    return (pd.concat(template, ignore_index=True))# return a dataframe with our data of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e047636-a586-4d61-b478-4278fd53e3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reader = pd.read_json('data/quotes-2020.json.bz2', lines=True, compression='bz2', chunksize=1000)\n",
    "for i, chunk in enumerate(df_reader):\n",
    "        chunk_clean=chunk_filtering(chunk, key_word) #recover interested row of the chunk\n",
    "        header = i == 0 #we kept the name of the column only for the first chunk\n",
    "        mode = 'w' if i == 0 else 'a' # For appending data to an existing CSV file (so for every chunk exepct the first one), \n",
    "                                        #we can use mode = a\n",
    "            \n",
    "        chunk_clean.to_csv(path_or_buf=\"data/clean_quotes-2020.bz2\",compression='bz2',header=header, mode=mode, index = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b58c3f6-6913-4e81-a678-b25818f59059",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1edcfe19-c424-4435-86bd-b0eb11096498",
   "metadata": {},
   "source": [
    "### Load Quotebank data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6264bc75-8b74-44f3-9e45-c079a54bea3b",
   "metadata": {},
   "source": [
    "First, let's recover the quotation of interest : as project is based on the caracterisation of the speaker, we decide to pre-select the quotations that are related to a speaker (i.e speaker value is different from 'None'). \n",
    "Moreover, we select the quotations whose subject is related to climate change : to do so we create a list of key word based on https://www.climaterealityproject.org/blog/key-terms-you-need-understand-climate-change and select quotes that contains at least one of these word.  (cf chunk_filtering method) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36cf40e0-63d6-4ca6-a98e-f167ebcc24e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#declaration of a key_world list\n",
    "key_word = [\"carbon dioxide\", \"greenhouse gas\", \"global warming\",\n",
    "             \"climate change\",  \"fossil fuels\", \"sea-level rise\",\n",
    "             \"renewable energy\", \"CO2\",\"methane\",\"PPM\",\"COP\",\"GIEC\", \n",
    "             \"biofuels\",\"business as usual\", \"carbon footprint\", \"carbon neutral\", \"carbon sequestration\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b031381d-12de-49d0-beb2-11debbae3192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_filtering(chunk, lst):\n",
    "    template=[] #creation of an empty list :it's always cheaper to append to a list and create a DataFrame than append on a empty dataframe.\n",
    "    for i in lst: \n",
    "        template.append(chunk.loc[chunk[\"quotation\"].apply(lambda x : i in x) & \n",
    "                                  chunk[\"speaker\"].apply(lambda x: x!= \"None\")].drop ()#select the quotation with value in speaker column different from 'None' \n",
    "                                                                                #and quotations containing the key word\n",
    "        \n",
    "    return (pd.concat(template, ignore_index=True))# return a dataframe with our data of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd22b34b-ce66-41f4-82ba-a234b2327cba",
   "metadata": {},
   "source": [
    "##### *2020 quotes extractions*\n",
    "The original dataset is of 792,3 Mo, so we decided to divide the dataset into chucks of 1000 rows and process each of them (by using the chunck_filtering). \n",
    "Then we load the process chunck into a new csv compressed bz2 file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28d4a71f-a51d-4a0b-a5c8-3fd15c6fa15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#maybe index = False ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54006733-37d4-49a3-b0b2-a72f200b825b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "with pd.read_json('data/quotes-2020.json.bz2', lines=True, compression='bz2', chunksize=1000) as df_reader : \n",
    "    for chunk in df_reader:\n",
    "        chunk_clean=chunk_filtering(chunk, key_word) #recover interested row of the chunk\n",
    "        header = i == 0 #we kept the name of the column only for the first chunk\n",
    "        mode = 'w' if i == 0 else 'a' # For appending data to an existing CSV file (so for every chunk exepct the first one), \n",
    "                                        #we can use mode = a\n",
    "            \n",
    "        chunk_clean.to_csv(path_or_buf=\"data/clean_quotes-2020.bz2\",compression='bz2',header=header, mode=mode)\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9edd5dc6-4e47-4744-94cc-5d0428f06a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes_2020= pd.read_csv('data/clean_quotes-2020.bz2', compression='bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f4e2620-72c8-4778-8767-4d6f46facabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " We extract 11091 quotes from the 2020 files\n"
     ]
    }
   ],
   "source": [
    "print( \" We extract {} quotes from the 2020 files\".format(len(quotes_2020)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae309716-2b59-4d84-8cac-259f2b256a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>quoteID</th>\n",
       "      <th>quotation</th>\n",
       "      <th>speaker</th>\n",
       "      <th>qids</th>\n",
       "      <th>date</th>\n",
       "      <th>numOccurrences</th>\n",
       "      <th>probas</th>\n",
       "      <th>urls</th>\n",
       "      <th>phase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-02-24-013709</td>\n",
       "      <td>For every doubling of carbon dioxide concentra...</td>\n",
       "      <td>E. Calvin Beisner</td>\n",
       "      <td>['Q19877395']</td>\n",
       "      <td>2020-02-24 16:02:23</td>\n",
       "      <td>1</td>\n",
       "      <td>[['E. Calvin Beisner', '0.677'], ['None', '0.3...</td>\n",
       "      <td>['https://www.heartland.org/news-opinion/news/...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-02-24-028340</td>\n",
       "      <td>If you're a doctor that cares about the wellbe...</td>\n",
       "      <td>Fiona Stanley</td>\n",
       "      <td>['Q1653736']</td>\n",
       "      <td>2020-02-24 12:45:00</td>\n",
       "      <td>4</td>\n",
       "      <td>[['Fiona Stanley', '0.9473'], ['None', '0.0527']]</td>\n",
       "      <td>['http://watoday.com.au/business/banking-and-f...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-24-004182</td>\n",
       "      <td>Also, spoke about a range of emerging sectors ...</td>\n",
       "      <td>Piyush Goyal</td>\n",
       "      <td>['Q7199798']</td>\n",
       "      <td>2020-01-24 19:02:14</td>\n",
       "      <td>2</td>\n",
       "      <td>[['Piyush Goyal', '0.6385'], ['Peter Voser', '...</td>\n",
       "      <td>['http://aninews.in/news/world/europe/piyush-g...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-29-062975</td>\n",
       "      <td>Many make the link today between their experie...</td>\n",
       "      <td>Peter Maurer</td>\n",
       "      <td>['Q117796', 'Q42426597']</td>\n",
       "      <td>2020-01-29 09:04:36</td>\n",
       "      <td>5</td>\n",
       "      <td>[['Peter Maurer', '0.8787'], ['None', '0.1213']]</td>\n",
       "      <td>['http://whbl.com/news/articles/2020/jan/29/hu...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-02-10-076321</td>\n",
       "      <td>the National Energy and Climate Plans are how ...</td>\n",
       "      <td>Kadri Simson</td>\n",
       "      <td>['Q13570003']</td>\n",
       "      <td>2020-02-10 05:51:51</td>\n",
       "      <td>1</td>\n",
       "      <td>[['Kadri Simson', '0.9269'], ['None', '0.0504'...</td>\n",
       "      <td>['https://www.politico.eu/newsletter/brussels-...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            quoteID  \\\n",
       "0           0  2020-02-24-013709   \n",
       "1           1  2020-02-24-028340   \n",
       "2           2  2020-01-24-004182   \n",
       "3           0  2020-01-29-062975   \n",
       "4           1  2020-02-10-076321   \n",
       "\n",
       "                                           quotation            speaker  \\\n",
       "0  For every doubling of carbon dioxide concentra...  E. Calvin Beisner   \n",
       "1  If you're a doctor that cares about the wellbe...      Fiona Stanley   \n",
       "2  Also, spoke about a range of emerging sectors ...       Piyush Goyal   \n",
       "3  Many make the link today between their experie...       Peter Maurer   \n",
       "4  the National Energy and Climate Plans are how ...       Kadri Simson   \n",
       "\n",
       "                       qids                 date  numOccurrences  \\\n",
       "0             ['Q19877395']  2020-02-24 16:02:23               1   \n",
       "1              ['Q1653736']  2020-02-24 12:45:00               4   \n",
       "2              ['Q7199798']  2020-01-24 19:02:14               2   \n",
       "3  ['Q117796', 'Q42426597']  2020-01-29 09:04:36               5   \n",
       "4             ['Q13570003']  2020-02-10 05:51:51               1   \n",
       "\n",
       "                                              probas  \\\n",
       "0  [['E. Calvin Beisner', '0.677'], ['None', '0.3...   \n",
       "1  [['Fiona Stanley', '0.9473'], ['None', '0.0527']]   \n",
       "2  [['Piyush Goyal', '0.6385'], ['Peter Voser', '...   \n",
       "3   [['Peter Maurer', '0.8787'], ['None', '0.1213']]   \n",
       "4  [['Kadri Simson', '0.9269'], ['None', '0.0504'...   \n",
       "\n",
       "                                                urls phase  \n",
       "0  ['https://www.heartland.org/news-opinion/news/...     E  \n",
       "1  ['http://watoday.com.au/business/banking-and-f...     E  \n",
       "2  ['http://aninews.in/news/world/europe/piyush-g...     E  \n",
       "3  ['http://whbl.com/news/articles/2020/jan/29/hu...     E  \n",
       "4  ['https://www.politico.eu/newsletter/brussels-...     E  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes_2020.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea50135-f229-42c6-91ce-52b28ec43bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ae4e97a-4d31-4c3f-a9ce-0c710073745d",
   "metadata": {},
   "source": [
    "##### *2019 quotes extractions*\n",
    "The original dataset is of 3.32 Go, so we decided to divide the dataset into chucks of 1000 rows and process each of them (by using the chunck_filtering). Then we load the process chunck into a new csv compressed bz2 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d516035-0364-44b3-b0c0-89a39d738e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "with pd.read_json('data/quotes-2019.json.bz2', lines=True, compression='bz2', chunksize=1000) as df_reader:\n",
    "    for chunk in df_reader:\n",
    "        chunk_clean=chunk_filtering(chunk, key_word) #recover interested row of the chunk\n",
    "        header = i == 0 #we kept the name of the column only for the first chunk\n",
    "        mode = 'w' if i == 0 else 'a' # For appending data to an existing CSV file (so for every chunk exepct the first one), \n",
    "                                        #we can use mode = a\n",
    "            \n",
    "        chunk_clean.to_csv(path_or_buf='data/clean_quotes-2019.bz2', compression='bz2', mode = mode, header=header) # create a new csv files compress with bz2 containing all the dataframe recover from the chunk; \n",
    "        i+=1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4947d82f-97dd-4d82-95ed-cd580d10f6c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/clean_quotes-2019.bz2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/dp/xm3y4vf11sd5mzn0yd618jhm0000gn/T/ipykernel_3385/1836899081.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mquotes_2019\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/clean_quotes-2019.bz2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bz2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/ada/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ada/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ada/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ada/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ada/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ada/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ada/lib/python3.8/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ada/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0;31m# BZ Compression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mcompression\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"bz2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m             handle = bz2.BZ2File(\n\u001b[0m\u001b[1;32m    659\u001b[0m                 \u001b[0;31m# Argument 1 to \"BZ2File\" has incompatible type \"Union[str,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m                 \u001b[0;31m# Union[IO[Any], RawIOBase, BufferedIOBase, TextIOBase, TextIOWrapper,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ada/lib/python3.8/bz2.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, buffering, compresslevel)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closefp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode_code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/clean_quotes-2019.bz2'"
     ]
    }
   ],
   "source": [
    "quotes_2019= pd.read_csv('data/clean_quotes-2019.bz2', compression='bz2') # load into the quotes_2019 df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93186176-a9be-4635-a87b-35816d8a7f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47280, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print( \" We extracted {} quotes from the 2019 files\".format(len(quotes_2019)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898e8ab3-00a4-4dc9-b89f-e3958ed2bc84",
   "metadata": {},
   "source": [
    "##### *2018 quotes extractions*\n",
    "The original dataset is of 4.48 Go, so we decided to divide the dataset into chucks of 1000 rows and process each of them ((by using the chunck_filtering). Then we load the process chunck into a new csv compressed bz2 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84914c33-8155-49ac-8d6a-d06d3f904e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "with pd.read_json('data/quotes-2018.json.bz2', lines=True, compression='bz2', chunksize=1000) as df_reader:\n",
    "    for chunk in df_reader:\n",
    "        \n",
    "        chunk_clean=chunk_filtering(chunk, key_word) #recover interested row of the chunk\n",
    "        header = i == 0 #we kept the name of the column only for the first chunk\n",
    "        mode = 'w' if i == 0 else 'a' # For appending data to an existing CSV file (so for every chunk exepct the first one), \n",
    "                                        #we can use mode = a\n",
    "            \n",
    "        chunk_clean.to_csv(path_or_buf='data/clean_quotes-2018.bz2', compression='bz2', mode = mode, header=header)\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca002f8c-56f4-4940-b198-a214ce2951f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes_2018= pd.read_csv('data/clean_quotes-2018.bz2', compression='bz2') #load the data to quotes_2018 df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f4203a5-49de-42f8-9168-6e97ef3b2602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " We extracted 35847 quotes from the 2018 files\n"
     ]
    }
   ],
   "source": [
    "print( \" We extracted {} quotes from the 2018 files\".format(len(quotes_2018)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8650499-8b64-4279-8cec-22c294b1f67c",
   "metadata": {},
   "source": [
    "##### *2017 quotes extractions*\n",
    "The original dataset is of 4.84 Go, so we decided to divide the dataset into chucks of 1000 rows and process each of them (by using the chunck_filtering). Then we load the process chunck into a new csv compressed bz2 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0c3dec8-5fa1-474c-9141-ea4e2fc85608",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "with pd.read_json('data/quotes-2017.json.bz2', lines=True, compression='bz2', chunksize=1000) as df_reader:\n",
    "    for chunk in df_reader:\n",
    "        chunk_clean=chunk_filtering(chunk, key_word) #recover interested row of the chunk\n",
    "        header = i == 0 #we kept the name of the column only for the first chunk\n",
    "        mode = 'w' if i == 0 else 'a' # For appending data to an existing CSV file (so for every chunk exepct the first one), \n",
    "                                        #we can use mode = a\n",
    "            \n",
    "        chunk_clean.to_csv(path_or_buf='data/clean_quotes-2017.bz2', compression='bz2', mode = 'a') # create a new csv files compress with bz2 containing all the dataframe recover from the chunk;\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fc42032-a525-4e90-9247-3a2e591aa26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes_2017= pd.read_csv('data/clean_quotes-2017.bz2', compression='bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c46c386-4866-431c-8fea-20707695e187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " We extracted 35324 quotes from the 2017 files\n"
     ]
    }
   ],
   "source": [
    "print( \" We extracted {} quotes from the 2017 files\".format(len(quotes_2017)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3274154-6ca0-4d95-a254-92ee85f4fd9a",
   "metadata": {},
   "source": [
    "##### *2016 quotes extractions*\n",
    "The original dataset is of 2.16 Go, so we decided to divide the dataset into chucks of 1000 rows and process each of them(by using the chunck_filtering). Then we load the process chunck into a new csv compressed bz2 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7dec2729-c0b5-419a-be2a-e39df7639e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "with pd.read_json('data/quotes-2016.json.bz2', lines=True, compression='bz2', chunksize=1000) as df_reader:\n",
    "    for chunk in df_reader:\n",
    "        chunk_clean=chunk_filtering(chunk, key_word) #recover interested row of the chunk\n",
    "        header = i == 0 #we kept the name of the column only for the first chunk\n",
    "        mode = 'w' if i == 0 else 'a' # For appending data to an existing CSV file (so for every chunk exepct the first one), \n",
    "                                        #we can use mode = a\n",
    "            \n",
    "        chunk_clean.to_csv(path_or_buf='data/clean_quotes-2016.bz2', compression='bz2', mode = 'a') # create a new csv files compress with bz2 containing all the dataframe recover from the chunk;\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31fc23d3-9713-4206-a126-54882bc6fc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes_2016= pd.read_csv('data/clean_quotes-2016.bz2', compression='bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "418d9ef6-3d36-42a4-a7ee-845d030cba7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " We extracted 18344 quotes from the 2016 files\n"
     ]
    }
   ],
   "source": [
    "print( \" We extracted {} quotes from the 2016 files\".format(len(quotes_2016)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf4b6c0-579e-49e4-ae1f-8096fdbf0821",
   "metadata": {},
   "source": [
    "##### *2015 quotes extractions*\n",
    "The original dataset is of 3.11 Go, so we decided to divide the dataset into chucks of 1000 rows and process each of them(by using the chunck_filtering). Then we load the process chunck into a new csv compressed bz2 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8131f70-68ac-4126-a31d-f37d8944f444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13829, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=0\n",
    "with pd.read_json('data/quotes-2015.json.bz2', lines=True, compression='bz2', chunksize=1000) as df_reader:\n",
    "    for chunk in df_reader:\n",
    "        chunk_clean=chunk_filtering(chunk, key_word) #recover interested row of the chunk\n",
    "        header = i == 0 #we kept the name of the column only for the first chunk\n",
    "        mode = 'w' if i == 0 else 'a' # For appending data to an existing CSV file (so for every chunk exepct the first one), \n",
    "                                        #we can use mode = a\n",
    "            \n",
    "        chunk_clean.to_csv(path_or_buf='data/clean_quotes-2015.bz2', compression='bz2', mode = 'a') # create a new csv files compress with bz2 containing all the dataframe recover from the chunk;\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba7620dc-490e-4311-b0cd-b188027133f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes_2015= pd.read_csv('data/clean_quotes-2015.bz2', compression='bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9150b06c-7c29-44e2-94e3-cecb975ac80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " We extracted 35176 quotes from the 2015 files\n"
     ]
    }
   ],
   "source": [
    "print( \" We extracted {} quotes from the 2015 files\".format(len(quotes_2015)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "12781cd5-f83d-4297-b58a-1e06a212bca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" At result, we extracted {} quotes fromes quotebank data\".format((len(quotes_2015)+len(quotes_2016)+len(quotes_2017)\n",
    "                                                                         +len(quotes_2018)+len(quotes_2019)+len(quotes_2020)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2ebf7f-c01e-443b-980a-354e326d4b9c",
   "metadata": {},
   "source": [
    "Now that we extracted the interesting data from the Quotebank data, let's add another dataset that will give us characteristic information about the speaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52a6a56-c643-4f00-8e53-72611156d0f5",
   "metadata": {},
   "source": [
    "## Load additional data Relative to speakers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5adca85-ec46-4904-b3c5-0395e659767a",
   "metadata": {},
   "source": [
    "The provided speaker_attributes.parquet file contains attributes in terms of QIDs, thereby being uninterpretable by humans (df_qid).\n",
    "To map the QIDs to meaningful labels, we used the provied wikidata_labels_descriptions_quotebank.csv.bz2 containg the labels and value fo the respective QID containing the df_qid (df_label_qid)\n",
    "By combaning the information of both we can obtained usefule information about speakers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "280f0f07-1673-4e62-a70f-81296b7db81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qid = pd.read_parquet(\"speaker_attributes.parquet\",engine= \"pyarrow\" )\n",
    "df_label_qid = pd.read_csv('data/wikidata_labels_descriptions_quotebank.csv.bz2', compression='bz2', index_col='QID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2db133-e8c3-43aa-ae20-07ded9ad40a7",
   "metadata": {},
   "source": [
    "Before extract the label of qid, let's check which column we want to keep in frame with our project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "34b98c8e-74a1-44c5-bc6e-395b22c4440b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aliases</th>\n",
       "      <th>date_of_birth</th>\n",
       "      <th>nationality</th>\n",
       "      <th>gender</th>\n",
       "      <th>lastrevid</th>\n",
       "      <th>ethnic_group</th>\n",
       "      <th>US_congress_bio_ID</th>\n",
       "      <th>occupation</th>\n",
       "      <th>party</th>\n",
       "      <th>academic_degree</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>candidacy</th>\n",
       "      <th>type</th>\n",
       "      <th>religion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Washington, President Washington, G. Washingt...</td>\n",
       "      <td>[+1732-02-22T00:00:00Z]</td>\n",
       "      <td>[Q161885, Q30]</td>\n",
       "      <td>[Q6581097]</td>\n",
       "      <td>1395141751</td>\n",
       "      <td>None</td>\n",
       "      <td>W000178</td>\n",
       "      <td>[Q82955, Q189290, Q131512, Q1734662, Q294126, ...</td>\n",
       "      <td>[Q327591]</td>\n",
       "      <td>None</td>\n",
       "      <td>Q23</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>[Q698073, Q697949]</td>\n",
       "      <td>item</td>\n",
       "      <td>[Q682443]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Douglas Noel Adams, Douglas Noël Adams, Dougl...</td>\n",
       "      <td>[+1952-03-11T00:00:00Z]</td>\n",
       "      <td>[Q145]</td>\n",
       "      <td>[Q6581097]</td>\n",
       "      <td>1395737157</td>\n",
       "      <td>[Q7994501]</td>\n",
       "      <td>None</td>\n",
       "      <td>[Q214917, Q28389, Q6625963, Q4853732, Q1884422...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Q42</td>\n",
       "      <td>Douglas Adams</td>\n",
       "      <td>None</td>\n",
       "      <td>item</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Paul Marie Ghislain Otlet, Paul Marie Otlet]</td>\n",
       "      <td>[+1868-08-23T00:00:00Z]</td>\n",
       "      <td>[Q31]</td>\n",
       "      <td>[Q6581097]</td>\n",
       "      <td>1380367296</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Q36180, Q40348, Q182436, Q1265807, Q205375, Q...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Q1868</td>\n",
       "      <td>Paul Otlet</td>\n",
       "      <td>None</td>\n",
       "      <td>item</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             aliases            date_of_birth  \\\n",
       "0  [Washington, President Washington, G. Washingt...  [+1732-02-22T00:00:00Z]   \n",
       "1  [Douglas Noel Adams, Douglas Noël Adams, Dougl...  [+1952-03-11T00:00:00Z]   \n",
       "2      [Paul Marie Ghislain Otlet, Paul Marie Otlet]  [+1868-08-23T00:00:00Z]   \n",
       "\n",
       "      nationality      gender   lastrevid ethnic_group US_congress_bio_ID  \\\n",
       "0  [Q161885, Q30]  [Q6581097]  1395141751         None            W000178   \n",
       "1          [Q145]  [Q6581097]  1395737157   [Q7994501]               None   \n",
       "2           [Q31]  [Q6581097]  1380367296         None               None   \n",
       "\n",
       "                                          occupation      party  \\\n",
       "0  [Q82955, Q189290, Q131512, Q1734662, Q294126, ...  [Q327591]   \n",
       "1  [Q214917, Q28389, Q6625963, Q4853732, Q1884422...       None   \n",
       "2  [Q36180, Q40348, Q182436, Q1265807, Q205375, Q...       None   \n",
       "\n",
       "  academic_degree     id              label           candidacy  type  \\\n",
       "0            None    Q23  George Washington  [Q698073, Q697949]  item   \n",
       "1            None    Q42      Douglas Adams                None  item   \n",
       "2            None  Q1868         Paul Otlet                None  item   \n",
       "\n",
       "    religion  \n",
       "0  [Q682443]  \n",
       "1       None  \n",
       "2       None  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_qid.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a524d4e3-29a4-49eb-b646-c6fc63e54220",
   "metadata": {},
   "source": [
    "Let's verify that academic_degree has revelant values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "293730b9-06ea-4bfd-909e-ab8a7cb806a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There's no academic degree revelant value ? False\n"
     ]
    }
   ],
   "source": [
    "print(\"There's no academic degree revelant value ? {}\".format(all(df_qid.academic_degree.isna())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573efeb6-dd9b-4121-8e10-d926f222795d",
   "metadata": {},
   "source": [
    "We decided to drop lastrevid, US_congress_bio_ID, type. Moreover, it's seems that academic_degree value are rare, let's check that too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "677d65e5-50ff-480a-b5ae-6f7d18551117",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qid.drop(['lastrevid', 'US_congress_bio_ID', 'type'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d2fb167-abb0-48cb-83fd-e9a514e60b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Q31</th>\n",
       "      <td>Belgium</td>\n",
       "      <td>country in western Europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q45</th>\n",
       "      <td>Portugal</td>\n",
       "      <td>country in southwestern Europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q75</th>\n",
       "      <td>Internet</td>\n",
       "      <td>global system of connected computer networks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q148</th>\n",
       "      <td>People's Republic of China</td>\n",
       "      <td>sovereign state in East Asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q155</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>country in South America</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Label                                   Description\n",
       "QID                                                                           \n",
       "Q31                      Belgium                     country in western Europe\n",
       "Q45                     Portugal                country in southwestern Europe\n",
       "Q75                     Internet  global system of connected computer networks\n",
       "Q148  People's Republic of China                  sovereign state in East Asia\n",
       "Q155                      Brazil                      country in South America"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_label_qid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6b7ee1f-81ec-4b1d-b038-7e89c44d3943",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We found out that some of the QIDs used in the speaker attribute file are actually redirection from an original QID. \n",
    "#We will manulally add their corresponding information using the orginal QID. We found the corespondance manualy between the two. \n",
    "#Here, there are in order, respectively the redirection QID, and its corresponding original one. One of he QID was only present \n",
    "#as a redirection, so we manually added this one (Q3186984), and its corresponding info. \n",
    "\n",
    "redirect_QID=['Q3268166', 'Q11815360', 'Q12014399', 'Q16287483',\n",
    "              'Q20432251', 'Q21550646', 'Q13365117', 'Q13424794',\n",
    "             'Q1248362', 'Q6859927', 'Q15145782',\n",
    "             'Q15991263', 'Q12455619', 'Q5568256', \n",
    "             'Q6363085', 'Q11819457', 'Q12334852', 'Q15145783']\n",
    "actual_QID=['Q1113899', 'Q1919436', 'Q250867', 'Q6051619',\n",
    "             'Q26934816', 'Q18431816', 'Q12840545', 'Q5157338',\n",
    "            'Q3455803', 'Q715222', 'Q1052281',\n",
    "            'Q2743689', 'Q7019111', 'Q3738699', \n",
    "            'Q380075', 'Q3391743', 'Q476246', 'Q2449503']\n",
    "\n",
    "#There is a QID that was deleted from Wikidata, Q99753484, so we will remove this QID later \n",
    "\n",
    "lst=[['Journalist', 'monthly magazine of the United Kingdom‘s National Union of Journalists (NUJ)']]\n",
    "indexes=['Q3186984']\n",
    "col=['Label', 'Description']\n",
    "for i in range(len(redirect_QID)):\n",
    "    lst.append([df_label_qid.loc[actual_QID[i]]['Label'], \n",
    "                df_label_qid.loc[actual_QID[i]]['Description']])\n",
    "    indexes.append(redirect_QID[i])\n",
    "\n",
    "additional_df= pd.DataFrame(lst, columns= col, index=indexes)\n",
    "df_label_qid_co=df_label_qid.append(additional_df, ignore_index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17875fab-7cbe-44b9-b911-2c7298b827e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that loops through a column to replace QID with their label, and skip None values. We will deal with them later, \n",
    "#when we will use the different data.\n",
    "def l(df) : \n",
    "    liste=[]\n",
    "    for row  in df: \n",
    "        if row is None: \n",
    "            continue #skip None values\n",
    "        template=[]\n",
    "        for value in row: #iterating over the values of a cell, as there are multiple QIDs in some of them.\n",
    "            if value == 'Q99753484': #To filter the deleted QID\n",
    "                continue\n",
    "            template.append(df_label_qid_co.loc[value]['Label']) #Map the QID to its corresponding label. \n",
    "        liste.append(template)\n",
    "    return pd.Series(liste)\n",
    "#Applying the function to every column containing QIDs. \n",
    "df_qid [['nationality', 'gender', 'ethnic_group','occupation', 'party', 'academic_degree', 'candidacy', 'religion']] = df_qid[['nationality', 'gender', 'ethnic_group','occupation', 'party', 'academic_degree', 'candidacy', 'religion']].apply(l)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8a1db0-542d-4b9f-a27b-c780e2a03267",
   "metadata": {
    "tags": []
   },
   "source": [
    "# II- Filter the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1547a11-c37a-42a7-bc86-8abc812ef6e7",
   "metadata": {},
   "source": [
    "As a good data scientist, the first thing to do is to clean up the data : we need to filtered missing and duplicates rows if there are presented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e534141a-105f-48ba-9e87-daa98425ca57",
   "metadata": {},
   "source": [
    "*check for missing row*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a824dab5-3740-49b3-b29e-0f0b401546a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is there some missing rows ? False \n"
     ]
    }
   ],
   "source": [
    "print(\"Is there some missing rows ? {} \".format(np.array([quotes_2020.isnull().any(axis=1)]).all()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d751443c-4cb1-4ee9-9866-aaee5fde6d4c",
   "metadata": {},
   "source": [
    "##### *check for duplicate* #+verifier speakers+date\n",
    "We define a function that receive a dataframe (quotes_2020 ... quotes_2015) and remove their duplicates rows according to duplicate quotation;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e24b9c-41ef-4760-b305-4def17cb6faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicates (df): \n",
    "    \n",
    "    if df[\"quotation\"].is_unique  == False : \n",
    "        df.drop_duplicates(['quotation'], keep='first', inplace=True) #remove the duplicate rows directly on the df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75326db-260f-4ffa-ac6f-abcf76c54eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_duplicates(quotes_2020)\n",
    "check_duplicates(quotes_2019)\n",
    "check_duplicates(quotes_2018)\n",
    "check_duplicates(quotes_2017)\n",
    "check_duplicates(quotes_2016)\n",
    "check_duplicates(quotes_2017)\n",
    "check_duplicates(quotes_2015)\n",
    "print( \"We still get {} quotes from the 2020 dataset\".format(len(quotes_2020)))\n",
    "print( \"We still get {} quotes from the 2020 dataset\".format(len(quotes_2019)))\n",
    "print( \"We still get {} quotes from the 2020 dataset\".format(len(quotes_2018)))\n",
    "print( \"We still get {} quotes from the 2020 dataset\".format(len(quotes_2017)))\n",
    "print( \"We still get {} quotes from the 2020 dataset\".format(len(quotes_2016)))\n",
    "print( \"We still get {} quotes from the 2020 dataset\".format(len(quotes_2015)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71a57f9-a272-442e-8ed4-90f744832cfb",
   "metadata": {},
   "source": [
    "### Merge quotebank extracted data and caracteristics of the speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d753065-e3b6-4d26-be5f-e9a87b870fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>quoteID</th>\n",
       "      <th>quotation</th>\n",
       "      <th>speaker</th>\n",
       "      <th>qids</th>\n",
       "      <th>date</th>\n",
       "      <th>numOccurrences</th>\n",
       "      <th>probas</th>\n",
       "      <th>urls</th>\n",
       "      <th>phase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-02-24-013709</td>\n",
       "      <td>For every doubling of carbon dioxide concentra...</td>\n",
       "      <td>E. Calvin Beisner</td>\n",
       "      <td>['Q19877395']</td>\n",
       "      <td>2020-02-24 16:02:23</td>\n",
       "      <td>1</td>\n",
       "      <td>[['E. Calvin Beisner', '0.677'], ['None', '0.3...</td>\n",
       "      <td>['https://www.heartland.org/news-opinion/news/...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-02-24-028340</td>\n",
       "      <td>If you're a doctor that cares about the wellbe...</td>\n",
       "      <td>Fiona Stanley</td>\n",
       "      <td>['Q1653736']</td>\n",
       "      <td>2020-02-24 12:45:00</td>\n",
       "      <td>4</td>\n",
       "      <td>[['Fiona Stanley', '0.9473'], ['None', '0.0527']]</td>\n",
       "      <td>['http://watoday.com.au/business/banking-and-f...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-24-004182</td>\n",
       "      <td>Also, spoke about a range of emerging sectors ...</td>\n",
       "      <td>Piyush Goyal</td>\n",
       "      <td>['Q7199798']</td>\n",
       "      <td>2020-01-24 19:02:14</td>\n",
       "      <td>2</td>\n",
       "      <td>[['Piyush Goyal', '0.6385'], ['Peter Voser', '...</td>\n",
       "      <td>['http://aninews.in/news/world/europe/piyush-g...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-29-062975</td>\n",
       "      <td>Many make the link today between their experie...</td>\n",
       "      <td>Peter Maurer</td>\n",
       "      <td>['Q117796', 'Q42426597']</td>\n",
       "      <td>2020-01-29 09:04:36</td>\n",
       "      <td>5</td>\n",
       "      <td>[['Peter Maurer', '0.8787'], ['None', '0.1213']]</td>\n",
       "      <td>['http://whbl.com/news/articles/2020/jan/29/hu...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            quoteID  \\\n",
       "0           0  2020-02-24-013709   \n",
       "1           1  2020-02-24-028340   \n",
       "2           2  2020-01-24-004182   \n",
       "3           0  2020-01-29-062975   \n",
       "\n",
       "                                           quotation            speaker  \\\n",
       "0  For every doubling of carbon dioxide concentra...  E. Calvin Beisner   \n",
       "1  If you're a doctor that cares about the wellbe...      Fiona Stanley   \n",
       "2  Also, spoke about a range of emerging sectors ...       Piyush Goyal   \n",
       "3  Many make the link today between their experie...       Peter Maurer   \n",
       "\n",
       "                       qids                 date  numOccurrences  \\\n",
       "0             ['Q19877395']  2020-02-24 16:02:23               1   \n",
       "1              ['Q1653736']  2020-02-24 12:45:00               4   \n",
       "2              ['Q7199798']  2020-01-24 19:02:14               2   \n",
       "3  ['Q117796', 'Q42426597']  2020-01-29 09:04:36               5   \n",
       "\n",
       "                                              probas  \\\n",
       "0  [['E. Calvin Beisner', '0.677'], ['None', '0.3...   \n",
       "1  [['Fiona Stanley', '0.9473'], ['None', '0.0527']]   \n",
       "2  [['Piyush Goyal', '0.6385'], ['Peter Voser', '...   \n",
       "3   [['Peter Maurer', '0.8787'], ['None', '0.1213']]   \n",
       "\n",
       "                                                urls phase  \n",
       "0  ['https://www.heartland.org/news-opinion/news/...     E  \n",
       "1  ['http://watoday.com.au/business/banking-and-f...     E  \n",
       "2  ['http://aninews.in/news/world/europe/piyush-g...     E  \n",
       "3  ['http://whbl.com/news/articles/2020/jan/29/hu...     E  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test for 2020\n",
    "quotes_2020.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a85b60a-0362-42a1-b04e-5d6de53f2e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>quoteID</th>\n",
       "      <th>quotation</th>\n",
       "      <th>speaker</th>\n",
       "      <th>qids</th>\n",
       "      <th>date</th>\n",
       "      <th>numOccurrences</th>\n",
       "      <th>probas</th>\n",
       "      <th>urls</th>\n",
       "      <th>phase</th>\n",
       "      <th>...</th>\n",
       "      <th>ethnic_group</th>\n",
       "      <th>US_congress_bio_ID</th>\n",
       "      <th>occupation</th>\n",
       "      <th>party</th>\n",
       "      <th>academic_degree</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>candidacy</th>\n",
       "      <th>type</th>\n",
       "      <th>religion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-02-24-013709</td>\n",
       "      <td>For every doubling of carbon dioxide concentra...</td>\n",
       "      <td>E. Calvin Beisner</td>\n",
       "      <td>['Q19877395']</td>\n",
       "      <td>2020-02-24 16:02:23</td>\n",
       "      <td>1</td>\n",
       "      <td>[['E. Calvin Beisner', '0.677'], ['None', '0.3...</td>\n",
       "      <td>['https://www.heartland.org/news-opinion/news/...</td>\n",
       "      <td>E</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Q1607826]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Q19877395</td>\n",
       "      <td>E. Calvin Beisner</td>\n",
       "      <td>None</td>\n",
       "      <td>item</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-02-24-028340</td>\n",
       "      <td>If you're a doctor that cares about the wellbe...</td>\n",
       "      <td>Fiona Stanley</td>\n",
       "      <td>['Q1653736']</td>\n",
       "      <td>2020-02-24 12:45:00</td>\n",
       "      <td>4</td>\n",
       "      <td>[['Fiona Stanley', '0.9473'], ['None', '0.0527']]</td>\n",
       "      <td>['http://watoday.com.au/business/banking-and-f...</td>\n",
       "      <td>E</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Q13416803]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Q1653736</td>\n",
       "      <td>Fiona Stanley</td>\n",
       "      <td>None</td>\n",
       "      <td>item</td>\n",
       "      <td>[Q6423963]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-24-004182</td>\n",
       "      <td>Also, spoke about a range of emerging sectors ...</td>\n",
       "      <td>Piyush Goyal</td>\n",
       "      <td>['Q7199798']</td>\n",
       "      <td>2020-01-24 19:02:14</td>\n",
       "      <td>2</td>\n",
       "      <td>[['Piyush Goyal', '0.6385'], ['Peter Voser', '...</td>\n",
       "      <td>['http://aninews.in/news/world/europe/piyush-g...</td>\n",
       "      <td>E</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Q82955]</td>\n",
       "      <td>[Q10230]</td>\n",
       "      <td>None</td>\n",
       "      <td>Q7199798</td>\n",
       "      <td>Piyush Goyal</td>\n",
       "      <td>None</td>\n",
       "      <td>item</td>\n",
       "      <td>[Q9089]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-29-062975</td>\n",
       "      <td>Many make the link today between their experie...</td>\n",
       "      <td>Peter Maurer</td>\n",
       "      <td>['Q117796', 'Q42426597']</td>\n",
       "      <td>2020-01-29 09:04:36</td>\n",
       "      <td>5</td>\n",
       "      <td>[['Peter Maurer', '0.8787'], ['None', '0.1213']]</td>\n",
       "      <td>['http://whbl.com/news/articles/2020/jan/29/hu...</td>\n",
       "      <td>E</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Q42426597</td>\n",
       "      <td>Peter Maurer</td>\n",
       "      <td>None</td>\n",
       "      <td>item</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-29-062975</td>\n",
       "      <td>Many make the link today between their experie...</td>\n",
       "      <td>Peter Maurer</td>\n",
       "      <td>['Q117796', 'Q42426597']</td>\n",
       "      <td>2020-01-29 09:04:36</td>\n",
       "      <td>5</td>\n",
       "      <td>[['Peter Maurer', '0.8787'], ['None', '0.1213']]</td>\n",
       "      <td>['http://whbl.com/news/articles/2020/jan/29/hu...</td>\n",
       "      <td>E</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Q193391, Q82955]</td>\n",
       "      <td>[Q303745]</td>\n",
       "      <td>[Q959320]</td>\n",
       "      <td>Q117796</td>\n",
       "      <td>Peter Maurer</td>\n",
       "      <td>None</td>\n",
       "      <td>item</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49257</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-02-19-061285</td>\n",
       "      <td>Our colleagues will also continue to work toge...</td>\n",
       "      <td>Markus Dohle</td>\n",
       "      <td>['Q1901431']</td>\n",
       "      <td>2020-02-19 08:37:21</td>\n",
       "      <td>1</td>\n",
       "      <td>[['Markus Dohle', '0.7837'], ['None', '0.2163']]</td>\n",
       "      <td>['http://thebookseller.com/news/dohle-prh-well...</td>\n",
       "      <td>E</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Q2462658, Q2516866]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Q1901431</td>\n",
       "      <td>Markus Dohle</td>\n",
       "      <td>None</td>\n",
       "      <td>item</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49258</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-02-23-041538</td>\n",
       "      <td>We've been writing about climate change being ...</td>\n",
       "      <td>Lesley Hughes</td>\n",
       "      <td>['Q53473786']</td>\n",
       "      <td>2020-02-23 22:38:42</td>\n",
       "      <td>2</td>\n",
       "      <td>[['Lesley Hughes', '0.9023'], ['None', '0.0978']]</td>\n",
       "      <td>['https://www.nytimes.com/2020/02/23/world/aus...</td>\n",
       "      <td>E</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Q1650915]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Q93176067</td>\n",
       "      <td>Lesley Hughes</td>\n",
       "      <td>None</td>\n",
       "      <td>item</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49259</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-02-23-041538</td>\n",
       "      <td>We've been writing about climate change being ...</td>\n",
       "      <td>Lesley Hughes</td>\n",
       "      <td>['Q53473786']</td>\n",
       "      <td>2020-02-23 22:38:42</td>\n",
       "      <td>2</td>\n",
       "      <td>[['Lesley Hughes', '0.9023'], ['None', '0.0978']]</td>\n",
       "      <td>['https://www.nytimes.com/2020/02/23/world/aus...</td>\n",
       "      <td>E</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Q1930187]</td>\n",
       "      <td>[Q138345]</td>\n",
       "      <td>None</td>\n",
       "      <td>Q62575737</td>\n",
       "      <td>Lesley Hughes</td>\n",
       "      <td>[Q1466815]</td>\n",
       "      <td>item</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49260</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-02-23-041538</td>\n",
       "      <td>We've been writing about climate change being ...</td>\n",
       "      <td>Lesley Hughes</td>\n",
       "      <td>['Q53473786']</td>\n",
       "      <td>2020-02-23 22:38:42</td>\n",
       "      <td>2</td>\n",
       "      <td>[['Lesley Hughes', '0.9023'], ['None', '0.0978']]</td>\n",
       "      <td>['https://www.nytimes.com/2020/02/23/world/aus...</td>\n",
       "      <td>E</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Q1650915]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Q88142628</td>\n",
       "      <td>Lesley Hughes</td>\n",
       "      <td>None</td>\n",
       "      <td>item</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49261</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-02-23-041538</td>\n",
       "      <td>We've been writing about climate change being ...</td>\n",
       "      <td>Lesley Hughes</td>\n",
       "      <td>['Q53473786']</td>\n",
       "      <td>2020-02-23 22:38:42</td>\n",
       "      <td>2</td>\n",
       "      <td>[['Lesley Hughes', '0.9023'], ['None', '0.0978']]</td>\n",
       "      <td>['https://www.nytimes.com/2020/02/23/world/aus...</td>\n",
       "      <td>E</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Q864503]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Q53473786</td>\n",
       "      <td>Lesley Hughes</td>\n",
       "      <td>None</td>\n",
       "      <td>item</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49262 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0            quoteID  \\\n",
       "0               0  2020-02-24-013709   \n",
       "1               1  2020-02-24-028340   \n",
       "2               2  2020-01-24-004182   \n",
       "3               0  2020-01-29-062975   \n",
       "4               0  2020-01-29-062975   \n",
       "...           ...                ...   \n",
       "49257           1  2020-02-19-061285   \n",
       "49258           0  2020-02-23-041538   \n",
       "49259           0  2020-02-23-041538   \n",
       "49260           0  2020-02-23-041538   \n",
       "49261           0  2020-02-23-041538   \n",
       "\n",
       "                                               quotation            speaker  \\\n",
       "0      For every doubling of carbon dioxide concentra...  E. Calvin Beisner   \n",
       "1      If you're a doctor that cares about the wellbe...      Fiona Stanley   \n",
       "2      Also, spoke about a range of emerging sectors ...       Piyush Goyal   \n",
       "3      Many make the link today between their experie...       Peter Maurer   \n",
       "4      Many make the link today between their experie...       Peter Maurer   \n",
       "...                                                  ...                ...   \n",
       "49257  Our colleagues will also continue to work toge...       Markus Dohle   \n",
       "49258  We've been writing about climate change being ...      Lesley Hughes   \n",
       "49259  We've been writing about climate change being ...      Lesley Hughes   \n",
       "49260  We've been writing about climate change being ...      Lesley Hughes   \n",
       "49261  We've been writing about climate change being ...      Lesley Hughes   \n",
       "\n",
       "                           qids                 date  numOccurrences  \\\n",
       "0                 ['Q19877395']  2020-02-24 16:02:23               1   \n",
       "1                  ['Q1653736']  2020-02-24 12:45:00               4   \n",
       "2                  ['Q7199798']  2020-01-24 19:02:14               2   \n",
       "3      ['Q117796', 'Q42426597']  2020-01-29 09:04:36               5   \n",
       "4      ['Q117796', 'Q42426597']  2020-01-29 09:04:36               5   \n",
       "...                         ...                  ...             ...   \n",
       "49257              ['Q1901431']  2020-02-19 08:37:21               1   \n",
       "49258             ['Q53473786']  2020-02-23 22:38:42               2   \n",
       "49259             ['Q53473786']  2020-02-23 22:38:42               2   \n",
       "49260             ['Q53473786']  2020-02-23 22:38:42               2   \n",
       "49261             ['Q53473786']  2020-02-23 22:38:42               2   \n",
       "\n",
       "                                                  probas  \\\n",
       "0      [['E. Calvin Beisner', '0.677'], ['None', '0.3...   \n",
       "1      [['Fiona Stanley', '0.9473'], ['None', '0.0527']]   \n",
       "2      [['Piyush Goyal', '0.6385'], ['Peter Voser', '...   \n",
       "3       [['Peter Maurer', '0.8787'], ['None', '0.1213']]   \n",
       "4       [['Peter Maurer', '0.8787'], ['None', '0.1213']]   \n",
       "...                                                  ...   \n",
       "49257   [['Markus Dohle', '0.7837'], ['None', '0.2163']]   \n",
       "49258  [['Lesley Hughes', '0.9023'], ['None', '0.0978']]   \n",
       "49259  [['Lesley Hughes', '0.9023'], ['None', '0.0978']]   \n",
       "49260  [['Lesley Hughes', '0.9023'], ['None', '0.0978']]   \n",
       "49261  [['Lesley Hughes', '0.9023'], ['None', '0.0978']]   \n",
       "\n",
       "                                                    urls phase  ...  \\\n",
       "0      ['https://www.heartland.org/news-opinion/news/...     E  ...   \n",
       "1      ['http://watoday.com.au/business/banking-and-f...     E  ...   \n",
       "2      ['http://aninews.in/news/world/europe/piyush-g...     E  ...   \n",
       "3      ['http://whbl.com/news/articles/2020/jan/29/hu...     E  ...   \n",
       "4      ['http://whbl.com/news/articles/2020/jan/29/hu...     E  ...   \n",
       "...                                                  ...   ...  ...   \n",
       "49257  ['http://thebookseller.com/news/dohle-prh-well...     E  ...   \n",
       "49258  ['https://www.nytimes.com/2020/02/23/world/aus...     E  ...   \n",
       "49259  ['https://www.nytimes.com/2020/02/23/world/aus...     E  ...   \n",
       "49260  ['https://www.nytimes.com/2020/02/23/world/aus...     E  ...   \n",
       "49261  ['https://www.nytimes.com/2020/02/23/world/aus...     E  ...   \n",
       "\n",
       "      ethnic_group US_congress_bio_ID            occupation      party  \\\n",
       "0             None               None            [Q1607826]       None   \n",
       "1             None               None           [Q13416803]       None   \n",
       "2             None               None              [Q82955]   [Q10230]   \n",
       "3             None               None                  None       None   \n",
       "4             None               None     [Q193391, Q82955]  [Q303745]   \n",
       "...            ...                ...                   ...        ...   \n",
       "49257         None               None  [Q2462658, Q2516866]       None   \n",
       "49258         None               None            [Q1650915]       None   \n",
       "49259         None               None            [Q1930187]  [Q138345]   \n",
       "49260         None               None            [Q1650915]       None   \n",
       "49261         None               None             [Q864503]       None   \n",
       "\n",
       "       academic_degree         id              label   candidacy  type  \\\n",
       "0                 None  Q19877395  E. Calvin Beisner        None  item   \n",
       "1                 None   Q1653736      Fiona Stanley        None  item   \n",
       "2                 None   Q7199798       Piyush Goyal        None  item   \n",
       "3                 None  Q42426597       Peter Maurer        None  item   \n",
       "4            [Q959320]    Q117796       Peter Maurer        None  item   \n",
       "...                ...        ...                ...         ...   ...   \n",
       "49257             None   Q1901431       Markus Dohle        None  item   \n",
       "49258             None  Q93176067      Lesley Hughes        None  item   \n",
       "49259             None  Q62575737      Lesley Hughes  [Q1466815]  item   \n",
       "49260             None  Q88142628      Lesley Hughes        None  item   \n",
       "49261             None  Q53473786      Lesley Hughes        None  item   \n",
       "\n",
       "         religion  \n",
       "0            None  \n",
       "1      [Q6423963]  \n",
       "2         [Q9089]  \n",
       "3            None  \n",
       "4            None  \n",
       "...           ...  \n",
       "49257        None  \n",
       "49258        None  \n",
       "49259        None  \n",
       "49260        None  \n",
       "49261        None  \n",
       "\n",
       "[49262 rows x 25 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#quotes_2020.loc[len(quotes_2020.qids)==1]\n",
    "   \n",
    "#quotes_2020.loc[quotes_2020[\"qids\"].apply(lambda x : len(x.split())==1)]\n",
    "quotes_2020.merge(df_qid, left_on = 'speaker' ,right_on = 'label' , how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6d3d2ee-ac4f-4f3c-b516-49434253bc7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[                                        aliases            date_of_birth  \\\n",
       " 1142134  [Freeman John Dyson, Freeman J. Dyson]  [+1923-12-15T00:00:00Z]   \n",
       " \n",
       "          nationality      gender   lastrevid ethnic_group US_congress_bio_ID  \\\n",
       " 1142134  [Q145, Q30]  [Q6581097]  1339761020     [Q42406]               None   \n",
       " \n",
       "                                                 occupation party  \\\n",
       " 1142134  [Q170790, Q19350898, Q16742096, Q121594, Q169470]  None   \n",
       " \n",
       "         academic_degree       id          label candidacy  type religion  \n",
       " 1142134       [Q849697]  Q153717  Freeman Dyson      None  item     None  ,\n",
       " Empty DataFrame\n",
       " Columns: [aliases, date_of_birth, nationality, gender, lastrevid, ethnic_group, US_congress_bio_ID, occupation, party, academic_degree, id, label, candidacy, type, religion]\n",
       " Index: [],\n",
       "         aliases            date_of_birth nationality      gender   lastrevid  \\\n",
       " 6980860    None  [+1953-00-00T00:00:00Z]       [Q30]  [Q6581097]  1358124495   \n",
       " \n",
       "         ethnic_group US_congress_bio_ID             occupation     party  \\\n",
       " 6980860         None               None  [Q27827744, Q8125919]  [Q29468]   \n",
       " \n",
       "         academic_degree        id        label candidacy  type religion  \n",
       " 6980860            None  Q6948430  Myron Ebell      None  item     None  ,\n",
       "         aliases date_of_birth nationality gender  lastrevid ethnic_group  \\\n",
       " 1595492    None          None        None   None  975117192         None   \n",
       " \n",
       "         US_congress_bio_ID  occupation party academic_degree         id  \\\n",
       " 1595492               None  [Q1650915]  None            None  Q43293430   \n",
       " \n",
       "                  label candidacy  type religion  \n",
       " 1595492  Kiminori Itoh      None  item     None  ,\n",
       "                                                    aliases  \\\n",
       " 4537079  [I. Giaever, I Giaever, Giaever, Giaever I, Gi...   \n",
       " \n",
       "                    date_of_birth nationality      gender   lastrevid  \\\n",
       " 4537079  [+1929-04-05T00:00:00Z]  [Q20, Q30]  [Q6581097]  1356880375   \n",
       " \n",
       "         ethnic_group US_congress_bio_ID          occupation party  \\\n",
       " 4537079         None               None  [Q169470, Q121594]  None   \n",
       " \n",
       "         academic_degree       id         label candidacy  type religion  \n",
       " 4537079            None  Q192688  Ivar Giaever      None  item  [Q7066]  ,\n",
       " Empty DataFrame\n",
       " Columns: [aliases, date_of_birth, nationality, gender, lastrevid, ethnic_group, US_congress_bio_ID, occupation, party, academic_degree, id, label, candidacy, type, religion]\n",
       " Index: [],\n",
       "         aliases            date_of_birth nationality      gender   lastrevid  \\\n",
       " 4560828    None  [+1946-02-12T00:00:00Z]      [Q408]  [Q6581097]  1313706978   \n",
       " \n",
       "         ethnic_group US_congress_bio_ID occupation party academic_degree  \\\n",
       " 4560828         None               None  [Q520549]  None            None   \n",
       " \n",
       "               id       label candidacy  type religion  \n",
       " 4560828  Q945385  Ian Plimer      None  item     None  ,\n",
       " Empty DataFrame\n",
       " Columns: [aliases, date_of_birth, nationality, gender, lastrevid, ethnic_group, US_congress_bio_ID, occupation, party, academic_degree, id, label, candidacy, type, religion]\n",
       " Index: [],\n",
       "         aliases            date_of_birth nationality      gender   lastrevid  \\\n",
       " 3522326    None  [+1937-01-01T00:00:00Z]        None  [Q6581097]  1314782373   \n",
       " \n",
       "         ethnic_group US_congress_bio_ID occupation party academic_degree  \\\n",
       " 3522326         None               None  [Q188094]  None            None   \n",
       " \n",
       "                id        label candidacy  type religion  \n",
       " 3522326  Q4706328  Alan Carlin      None  item     None  ,\n",
       "                                                    aliases  \\\n",
       " 2274828  [Patrick J. Michaels, Patrick J Michaels, P. J...   \n",
       " \n",
       "                    date_of_birth nationality      gender   lastrevid  \\\n",
       " 2274828  [+1950-02-15T00:00:00Z]       [Q30]  [Q6581097]  1362576478   \n",
       " \n",
       "         ethnic_group US_congress_bio_ID             occupation party  \\\n",
       " 2274828         None               None  [Q11986654, Q1622272]  None   \n",
       " \n",
       "         academic_degree       id             label candidacy  type religion  \n",
       " 2274828            None  Q206017  Patrick Michaels      None  item     None  ]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst = ['Freeman Dyson', 'Bjorn Lomborg', 'Myron Ebell', 'Kiminori Itoh', 'Ivar Giaever', \n",
    "       'Will Happer', 'Ian Plimer', 'Michael Chrichton', 'Alan Carlin', 'Patrick Michaels']\n",
    "tp=[]\n",
    "for i in lst  : \n",
    "    tp.append(df_qid.loc[df_qid['label'].apply(lambda x : i == x)])\n",
    "pd.concat(tp, ignore_index=True)    \n",
    "\n",
    "tp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a8f58c-5c08-44d5-81fc-151fdec345a0",
   "metadata": {},
   "source": [
    "## I-Exploration of our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5271ef-c0c5-480c-adb8-f79a0d822615",
   "metadata": {},
   "source": [
    "Let's see some distribution and statitics: \n",
    " - aged people vs yound people \n",
    " - party politics  \n",
    "ect... \n",
    "\n",
    "stat : correlation coeff ; m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5756638-5cba-49a1-82a2-9c06b7363eda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
